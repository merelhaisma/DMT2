{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:55.772067300Z",
     "start_time": "2023-05-22T18:30:55.761883300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:57.563630Z",
     "start_time": "2023-05-22T18:30:55.775424100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into pandas dataframes. The data has to be manually saved to a folder called 'data'.\n",
    "# Note: the data is quite large, so this may take a while (~40 seconds) if the data is read as a csv. To speed up further file reading, it is converted to pickle format.\n",
    "if (os.path.exists(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    & os.path.exists((os.path.join('data', 'test_set_VU_DM.pickle')))):\n",
    "    train_df = pd.read_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df = pd.read_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "else:\n",
    "    train_df = pd.read_csv(os.path.join('data', 'training_set_VU_DM.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('data', 'test_set_VU_DM.csv'))\n",
    "    train_df.to_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df.to_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "\n",
    "# train_df = pd.read_csv(os.path.join('data', 'dummy_training.csv'))\n",
    "# test_df = pd.read_csv(os.path.join('data', 'dummy_testing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.444183800Z",
     "start_time": "2023-05-22T18:30:57.596519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of data collection: 2012/11/01 - 2013/06/29\n",
      "Train data contains 4,999 rows and 54 columns\n",
      "Test data contains 4,999 rows and 50 columns\n",
      "\n",
      "Train data:\n",
      "Number of unique search IDs: 207\n",
      "Number of unique property IDs: 4,602\n",
      "Number of clicks per search: avg. 1.09, std. 0.21\n",
      "Number of bookings per search: avg. 0.66, std. 0.16\n",
      "\n",
      "Test data:\n",
      "Number of unique search IDs: 194\n",
      "Number of unique property IDs: 4,551\n"
     ]
    }
   ],
   "source": [
    "print(f\"Period of data collection: {pd.to_datetime(train_df['date_time']).min().strftime('%Y/%m/%d')} - {pd.to_datetime(train_df['date_time']).max().strftime('%Y/%m/%d')}\")\n",
    "print(f\"Train data contains {train_df.shape[0]:,} rows and {train_df.shape[1]} columns\")\n",
    "print(f\"Test data contains {test_df.shape[0]:,} rows and {test_df.shape[1]} columns\")\n",
    "print()\n",
    "print(f\"Train data:\")\n",
    "print(f\"Number of unique search IDs: {len(train_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(train_df['prop_id'].unique()):,}\")\n",
    "print(f\"Number of clicks per search: avg. {train_df['click_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['click_bool'].std():.2f}\")\n",
    "print(f\"Number of bookings per search: avg. {train_df['booking_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['booking_bool'].std():.2f}\")\n",
    "print()\n",
    "print(f\"Test data:\")\n",
    "print(f\"Number of unique search IDs: {len(test_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(test_df['prop_id'].unique()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.451457800Z",
     "start_time": "2023-05-22T18:30:59.450452700Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create the following new features: has_starrating, has_review_score, traveling_abroad, srch_prop_country_match, month, and day_of_week\n",
    "    \"\"\"\n",
    "    # has_starrating: boolean whether prop_starrating is 0 or null\n",
    "    df[\"has_starrating\"] = df[\"prop_starrating\"].isnull()\n",
    "    df[\"has_starrating\"] = df[\"has_starrating\"].astype(int)\n",
    "    df.loc[df[\"prop_starrating\"] == 0, \"has_starrating\"] = 1\n",
    "\n",
    "    # has_review_score: boolean whether prop_review_score is 0 or null\n",
    "    df[\"has_review_score\"] = df[\"prop_review_score\"].isnull()\n",
    "    df[\"has_review_score\"] = df[\"has_review_score\"].astype(int)\n",
    "    df.loc[df[\"prop_review_score\"] == 0, \"has_review_score\"] = 1\n",
    "\n",
    "    # traveling_abroad: boolean whether visitor_location_country_id != prop_country_id\n",
    "    df[\"traveling_abroad\"] = df[\"visitor_location_country_id\"] != df[\"prop_country_id\"]\n",
    "    df[\"traveling_abroad\"] = df[\"traveling_abroad\"].astype(int)\n",
    "\n",
    "    # srch_prop_country_match: boolean whether srch_destination_id == prop_country_id\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_destination_id\"] == df[\"prop_country_id\"]\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_prop_country_match\"].astype(int)\n",
    "\n",
    "    # month: month of the search, one-hot encoded\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"month\"] = df[\"date_time\"].dt.month\n",
    "    df[\"month\"] = df[\"month\"].map({1: \"jan\", 2: \"feb\", 3: \"mar\", 4: \"apr\", 5: \"may\", 6: \"jun\", 7: \"jul\", 8: \"aug\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dec\"})\n",
    "    df = pd.get_dummies(df, columns=[\"month\"], dtype=int)\n",
    "    for col in [\"month_jan\", \"month_feb\", \"month_mar\", \"month_apr\", \"month_may\", \"month_jun\", \"month_jul\", \"month_aug\", \"month_sep\", \"month_oct\", \"month_nov\", \"month_dec\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # day_of_week: day of the week of the search\n",
    "    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n",
    "    df[\"day_of_week\"] = df[\"day_of_week\"].map({0: \"mon\", 1: \"tue\", 2: \"wed\", 3: \"thu\", 4: \"fri\", 5: \"sat\", 6: \"sun\"})\n",
    "    df = pd.get_dummies(df, columns=[\"day_of_week\"], dtype=int)\n",
    "    for col in [\"day_of_week_mon\", \"day_of_week_tue\", \"day_of_week_wed\", \"day_of_week_thu\", \"day_of_week_fri\", \"day_of_week_sat\", \"day_of_week_sun\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.456697Z",
     "start_time": "2023-05-22T18:30:59.455690200Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_nan_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\", and \"compx_rate_percent_diff\" have >90% NaN values. These values cannot be imputed accurately, so we drop these columns.\n",
    "    \"\"\"\n",
    "    cols = [\"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\"] + [f\"comp{i}_rate_percent_diff\" for i in range(1, 9)]\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.463348700Z",
     "start_time": "2023-05-22T18:30:59.462342100Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values for the following columns: \"prop_starrating\", \"prop_review_score\", \"compx_rate\", and \"compx_inv\".\n",
    "\n",
    "    For \"prop_starrating\" and \"prop_review_score\", we replace 0 values with NaN and then impute the NaN values with the mean per srch_id. Remaining NaN values are then filled with 0.\n",
    "    For \"compx_rate\" and \"compx_inv\", we assume that missing data means that Expedia has the same price and equal availability as its competitors. We therefore impute the NaN values with 0.\n",
    "    \"\"\"\n",
    "    # Replace 0 values with NaN\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].replace(0, np.nan)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].replace(0, np.nan)\n",
    "    # Impute NaN values with mean per srch_id\n",
    "    df[\"prop_starrating\"] = df.groupby(\"srch_id\")[\"prop_starrating\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    df[\"prop_review_score\"] = df.groupby(\"srch_id\")[\"prop_review_score\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    # Fill remaining NaN values with 0\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].fillna(0)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].fillna(0)\n",
    "\n",
    "    # Impute NaN values with 0\n",
    "    for i in range(1, 9):\n",
    "        df[f\"comp{i}_rate\"] = df[f\"comp{i}_rate\"].fillna(0)\n",
    "        df[f\"comp{i}_inv\"] = df[f\"comp{i}_inv\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.470380900Z",
     "start_time": "2023-05-22T18:30:59.468366600Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_aggregated_values(df):\n",
    "    \"\"\"\n",
    "    Compute the mean, median and standard deviation for the following columns:\n",
    "    \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "    \"\"\"\n",
    "    numerical_cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    # srch_length_of_stay, srch_booking_window, srch_adults_count, srch_children_count, and srch_room_count are also numerical variables, but it has no use aggregating these values over prop_id.\n",
    "\n",
    "    agg_df = df.groupby(\"prop_id\").agg({col: [\"mean\", \"std\", \"median\"] for col in numerical_cols})\n",
    "    agg_df.columns = [\"_\".join(col) for col in agg_df.columns]\n",
    "    agg_df.fillna(0, inplace=True)  # Fill standard deviation NaN values with 0\n",
    "    for col in agg_df.columns:\n",
    "        df[col] = df[\"prop_id\"].map(agg_df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.497541600Z",
     "start_time": "2023-05-22T18:30:59.473404100Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_relative_values(df):\n",
    "    \"\"\"\n",
    "    Subtract the mean per srch_id from the following columns and make them into new columns:\n",
    "    \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "\n",
    "    This is done so that the model can learn the relative values of these columns per srch_id.\n",
    "    \"\"\"\n",
    "    cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    grouper = df.groupby('srch_id')\n",
    "    for col in cols:\n",
    "        df[f\"relative_{col}\"] = df[col] - grouper[col].transform('mean')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.477989600Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that for \"orig_destination_distance\" over 75% of the data with a calculated value lower than 0.95 of the largest distance was lower than 130, meaning that the distance per srch_id is roughly the same. We assume therefore that this is not a deciding factor for a customer in their booking process and drop this column.\n",
    "\n",
    "    Features were created from \"date_time\" and the column will not be used anymore, so we drop this column as well.\n",
    "\n",
    "    Columns containing IDs (\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"prop_id\", and \"srch_destination_id\") are not used in the model, so we drop these columns as well. \"srch_id\" and \"prop_id\" will remain in the columns for now for later use.\n",
    "\n",
    "    If the supplied dataframe is the training dataframe, drop the unused target columns as well.\n",
    "\n",
    "    # TODO:\n",
    "    I don't really know what to do with \"prop_location_score2\" yet, so I'll drop it for now.\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"orig_destination_distance\"], inplace=True)\n",
    "    df.drop(columns=[\"date_time\"], inplace=True)\n",
    "    df.drop(columns=[\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"srch_destination_id\"], inplace=True)\n",
    "    df.drop(columns=[\"prop_location_score2\"], inplace=True)\n",
    "    for col in [\"position\", \"gross_bookings_usd\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.483040900Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(train_df: pd.DataFrame, test_df: pd.DataFrame, target_value: str = \"booking_bool\", train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Process the dataframes for training and testing the model.\n",
    "    :param train_df: The training dataframe\n",
    "    :param test_df: The test dataframe\n",
    "    :param target_value: The target value to use for training the model. Either \"booking_bool\" or \"both\".\n",
    "    :return: The processed dataframes\n",
    "    \"\"\"\n",
    "    # Create features\n",
    "    train_df = create_features(train_df)\n",
    "    test_df = create_features(test_df)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    train_df = drop_nan_columns(train_df)\n",
    "    test_df = drop_nan_columns(test_df)\n",
    "\n",
    "    # Impute missing values\n",
    "    train_df = impute_missing_values(train_df)\n",
    "    test_df = impute_missing_values(test_df)\n",
    "\n",
    "    # Compute aggregated values\n",
    "    train_df = compute_aggregated_values(train_df)\n",
    "    test_df = compute_aggregated_values(test_df)\n",
    "\n",
    "    # Compute relative values\n",
    "    train_df = compute_relative_values(train_df)\n",
    "    test_df = compute_relative_values(test_df)\n",
    "\n",
    "    # Drop columns\n",
    "    train_df = drop_columns(train_df)\n",
    "    test_df = drop_columns(test_df)\n",
    "\n",
    "    # Split train data into features and target\n",
    "    if target_value == \"booking_bool\":\n",
    "        train_df[\"target\"] = train_df[\"booking_bool\"]\n",
    "    elif target_value == \"both\":\n",
    "        train_df[\"target\"] = 5*train_df[\"booking_bool\"] + train_df[\"click_bool\"]\n",
    "    else:\n",
    "        raise ValueError(\"target_value must be either 'booking_bool' or 'both'\")\n",
    "\n",
    "    train_df.drop(columns=[\"booking_bool\", \"click_bool\"], inplace=True)\n",
    "\n",
    "    train_df = train_df[:int(train_frac * len(train_df))]\n",
    "    val_df = train_df[int(train_frac * len(train_df)):]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.496014200Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences(df: pd.DataFrame, kind: str = \"train\") -> dict:\n",
    "    \"\"\"\n",
    "    Generate sequences from the dataframe.\n",
    "    :param df: The dataframe to take the sequences from.\n",
    "    :param kind: The kind of dataframe. Either \"train\", \"val\" or \"test\".\n",
    "    :return: Dictionary containing the sequences and targets for the training data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    if kind == \"train\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\", \"target\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Train DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values, \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"val\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\", \"target\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Validation DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values, \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"test\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Test DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values})\n",
    "    else:\n",
    "        raise ValueError(\"kind must be either 'train', 'val' or 'test'\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.512486500Z",
     "start_time": "2023-05-22T18:30:59.508476500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_lgb_sequences(train_df, test_df, force_data_processing=False, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Generate the train, validation and test dictionaries\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join('data', f'train_sequences_both.pickle')) \\\n",
    "            & os.path.exists(os.path.join('data', f'test_sequences_both.pickle')) \\\n",
    "            & os.path.exists(os.path.join('data', f'val_sequences_both.pickle')) \\\n",
    "            & (not force_data_processing):\n",
    "        print(f\"Loading train and test sequences from pickle files...\")\n",
    "        with open(os.path.join('data', f'train_sequences_both.pickle'), 'rb') as f:\n",
    "            train_sequences = pickle.load(f)\n",
    "        with open(os.path.join('data', f'val_sequences_both.pickle'), 'rb') as f:\n",
    "            val_sequences = pickle.load(f)\n",
    "        with open(os.path.join('data', f'test_sequences_both.pickle'), 'rb') as f:\n",
    "            test_sequences = pickle.load(f)\n",
    "    else:\n",
    "        processed_train_df, processed_val_df, processed_test_df = process_data(train_df, test_df, target_value=\"both\", train_frac=train_frac)\n",
    "        train_sequences = generate_sequences(processed_train_df, kind=\"train\")\n",
    "        with open(os.path.join('data', f'train_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(train_sequences, f)\n",
    "        val_sequences = generate_sequences(processed_val_df, kind=\"val\")\n",
    "        with open(os.path.join('data', f'val_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(val_sequences, f)    \n",
    "        test_sequences = generate_sequences(processed_test_df, kind=\"test\")\n",
    "        with open(os.path.join('data', f'test_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(test_sequences, f)\n",
    "\n",
    "    return train_sequences, val_sequences, test_sequences\n",
    "\n",
    "def generate_lgb_data(sequences, kind=\"train\"):\n",
    "    x_data = np.array([seq[\"sequence\"] for seq in sequences])\n",
    "    \n",
    "    query_to_properties = {}\n",
    "    query_to_properties_to_target = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        \n",
    "        srch_id = seq[\"srch_id\"]\n",
    "        prop_id = seq[\"prop_id\"]\n",
    "        # Add the search ID to the dictionary if it isn't in already\n",
    "        if srch_id not in query_to_properties.keys():\n",
    "            query_to_properties[srch_id] = []\n",
    "            query_to_properties_to_target[srch_id] = {}\n",
    "        \n",
    "        query_to_properties[srch_id].append(prop_id)\n",
    "        \n",
    "        if kind ==\"train\" or kind==\"val\":\n",
    "            query_to_properties_to_target[srch_id][prop_id] = seq[\"target\"]\n",
    "        \n",
    "    # When generating train data, return also targets\n",
    "    if kind==\"train\" or kind==\"val\":\n",
    "        y_data = np.array([seq[\"target\"] for seq in sequences])\n",
    "\n",
    "        return x_data, y_data, query_to_properties, query_to_properties_to_target\n",
    "\n",
    "    # When generating test data, return only features and query information\n",
    "    elif kind==\"test\":\n",
    "\n",
    "        return x_data, query_to_properties\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset creation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the LambdaMART model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:22.803150600Z",
     "start_time": "2023-05-22T18:30:59.514486500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequences...\n",
      "Loading train and test sequences from pickle files...\n",
      "Done generating sequences. This took 0.03s\n",
      "Generating train features...\n",
      "Done generating train features. This took 0.01s\n",
      "Generating validation features...\n",
      "Done generating validation features. This took 0.00s\n",
      "Generating test features...\n",
      "Done generating test features. This took 0.00s\n",
      "Transforming train features into appropriate dataset...\n",
      "Done generating dataset. This took 0.00s\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "train_frac = 0.9\n",
    "\n",
    "# Assume X_train and y_train are the training features and target values, respectively\n",
    "print(\"Generating sequences...\")\n",
    "t = time.time()\n",
    "lgb_train_sequences, lgb_val_sequences, lgb_test_sequences = generate_lgb_sequences(train_df, test_df, force_data_processing=False, train_frac=train_frac)\n",
    "print(f\"Done generating sequences. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating train features...\")\n",
    "train_x, train_y, train_queries, train_props2targets = generate_lgb_data(lgb_train_sequences, kind=\"train\")\n",
    "print(f\"Done generating train features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating validation features...\")\n",
    "val_x, val_y, val_queries, val_props2targets = generate_lgb_data(lgb_val_sequences, kind=\"val\")\n",
    "print(f\"Done generating validation features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating test features...\")\n",
    "test_x, test_queries = generate_lgb_data(lgb_test_sequences, kind=\"test\")\n",
    "print(f\"Done generating test features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Transforming train features into appropriate dataset...\")\n",
    "# Create a LightGBM dataset from the training data\n",
    "train_data = lgb.Dataset(train_x, label=train_y, group=[len(elem) for elem in train_queries.values()])\n",
    "print(f\"Done generating dataset. This took {time.time() - t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:22.999267400Z",
     "start_time": "2023-05-22T19:04:22.922215900Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_and_write(fn, predictions, queries):\n",
    "    \"\"\"\n",
    "    Function to write a model's predictions to disk, where queries is a dictionary mapping\n",
    "    a query ID to a list of property IDs, and predictions is an (ordered!) list containing\n",
    "    the output of the model. \n",
    "    \"\"\"\n",
    "    srch_to_ranks = {}\n",
    "    \n",
    "    i=0\n",
    "    for query_id, property_ids in tqdm(queries.items()):\n",
    "        \n",
    "        # Retrieve all probabilities for a certain query ID (assuming the data is still ordered)\n",
    "        ranks = predictions[i:i+len(property_ids)]\n",
    "\n",
    "        # Keep track of the probabilities\n",
    "        srch_to_ranks[query_id] = ranks\n",
    "        i+=len(property_ids)\n",
    "    \n",
    "    # Write predictions to file\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(\"srch_id,prop_id\\n\")\n",
    "\n",
    "        # Iterate over all different queries\n",
    "        for srch_id, srch_probs in srch_to_ranks.items():\n",
    "            \n",
    "            # Get the order of the hotels based on the predicted probabilities\n",
    "            sorted_indices = np.array(srch_probs).argsort()[::-1]\n",
    "            property_ids_sorted = np.array(queries[srch_id])[np.array(sorted_indices)]\n",
    "\n",
    "            for s_id, p_id in zip([srch_id for _ in range(len(property_ids_sorted))],property_ids_sorted):\n",
    "                f.write(f\"{s_id},{p_id}\\n\")\n",
    "\n",
    "def write_predictions(fn, predictions):\n",
    "    \"\"\"\n",
    "    Function to write a models predictions to disk\n",
    "    \"\"\"\n",
    "\n",
    "    with open(fn, \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:37:21.306745600Z",
     "start_time": "2023-05-22T20:37:21.302218900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate NDCG@k score from file\n",
    "\n",
    "def NDCG_from_file(fn, prop2targets, k):\n",
    "    \"\"\"\n",
    "    Function to calculate the NDCG@k score from a txt file in the format specified in the assignment (two \n",
    "    columns, the left one containing the search IDs and the right one containing the PropertyIDs, sorted by\n",
    "    relevance)\n",
    "    \"\"\"\n",
    "    query_to_properties = {}\n",
    "\n",
    "    # Fill the query-to-properties dictionary\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()[1:] # Remove the header\n",
    "        for line in lines:\n",
    "            query_id, prop_id = line.split(\",\")\n",
    "            query_id, prop_id = int(query_id), int(prop_id)\n",
    "            if query_id not in query_to_properties.keys():\n",
    "                query_to_properties[query_id] = [prop_id]\n",
    "            else:\n",
    "                query_to_properties[query_id].append(prop_id)\n",
    "\n",
    "    NDCG_score = 0\n",
    "    for query_id, prop_ids in query_to_properties.items():\n",
    "        # Get all scores in order how they are scored by the model\n",
    "        all_scores = np.array([prop2targets[query_id][prop_id] for prop_id in prop_ids])\n",
    "\n",
    "        # Take only the first k elements\n",
    "        true_scores = all_scores[:k]\n",
    "\n",
    "        # Calculate DCG@k (Discounted Cumulative Gain at k)\n",
    "        dcg = np.sum(true_scores / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Sort the true scores in descending order\n",
    "        true_sorted_scores = np.sort(all_scores)[:k][::-1]\n",
    "        \n",
    "        # Calculate ideal DCG@k\n",
    "        ideal_dcg = np.sum(true_sorted_scores[:k] / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Calculate NDCG@k\n",
    "        NDCG_score += (dcg / ideal_dcg) if ideal_dcg > 0 else 0.0\n",
    "\n",
    "    return NDCG_score / len(query_to_properties.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.003856900Z",
     "start_time": "2023-05-22T19:04:22.942120800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the parameters for the LambdaMART model\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],  # Evaluation at NDCG@5\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'device': 'gpu' # Change to GPU if available\n",
    "}\n",
    "\n",
    "# TODO: check what hyperparameters winners used and optimize\n",
    "gbm_params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "num_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.003856900Z",
     "start_time": "2023-05-22T19:04:22.943117200Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train the LambdaMART model\n",
    "\n",
    "# t = time.time()\n",
    "# print(\"Training LambdaMART model...\")\n",
    "# vanilla_mart_model = lgb.train(params, train_data, num_rounds)\n",
    "# print(f\"Done training LambdaMART model. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "# # Save the trained model\n",
    "# vanilla_mart_model.save_model('lambda_mart_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.390592100Z",
     "start_time": "2023-05-22T19:04:22.943117200Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Training GBM model...\")\n",
    "# GBM_model = lgb.train(gbm_params, train_data, num_rounds)\n",
    "# print(f\"Done training GBM model. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "# # Save the trained model\n",
    "# GBM_model.save_model('gbm_model.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training of the ensemble model**\n",
    "\n",
    "This one should work the best but I kept the code above just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:08:44.712089Z",
     "start_time": "2023-05-22T19:04:22.948707800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble of GBM models\n",
      "Training model 0\n",
      "Training model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyproxius\\anaconda3\\envs\\dmt\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2\n",
      "Training model 3\n",
      "Training model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 17988.44it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 193829.20it/s]\n",
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<?, ?it/s]\n",
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 193875.38it/s]\n",
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<?, ?it/s]\n",
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<?, ?it/s]\n",
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"test_output/GBM\"):\n",
    "    os.makedirs(\"test_output/GBM\")\n",
    "\n",
    "if not os.path.isdir(\"raw_outputs/GBM/individual_GBMs/val\"):\n",
    "    os.makedirs(\"raw_outputs/GBM/individual_GBMs/val\")\n",
    "\n",
    "if not os.path.isdir(\"raw_outputs/GBM/ensemble_GBMs/val\"):\n",
    "    os.makedirs(\"raw_outputs/GBM/ensemble_GBMs/val\")\n",
    "\n",
    "if not os.path.isdir(\"raw_outputs/GBM/individual_GBMs/test\"):\n",
    "    os.makedirs(\"raw_outputs/GBM/individual_GBMs/test\")\n",
    "\n",
    "if not os.path.isdir(\"raw_outputs/GBM/ensemble_GBMs/test\"):\n",
    "    os.makedirs(\"raw_outputs/GBM/ensemble_GBMs/test\")\n",
    "\n",
    "# Define the number of models in the ensemble\n",
    "num_models = 5\n",
    "\n",
    "# Train the ensemble of GBM models\n",
    "models = []\n",
    "print(f\"Training ensemble of GBM models\")\n",
    "for i in range(num_models):\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(42 + i)\n",
    "    \n",
    "    # Create a random subsample of the training data\n",
    "    subsample_indices = np.random.choice(len(train_x), size=len(train_x), replace=True)\n",
    "    subsample_data = train_data.subset(subsample_indices)\n",
    "    \n",
    "    # Train an individual GBM model\n",
    "    print(f\"Training model {i}\")\n",
    "    model = lgb.train(gbm_params, subsample_data, num_boost_round=100)\n",
    "    \n",
    "    # Add the trained model to the ensemble\n",
    "    models.append(model)\n",
    "\n",
    "# Make predictions using the ensemble of models\n",
    "ensemble_predictions_val = np.zeros(len(val_x))\n",
    "ensemble_predictions_test = np.zeros(len(test_x))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    predictions_val = model.predict(val_x)\n",
    "    predictions_test = model.predict(test_x)\n",
    "    \n",
    "    write_predictions(f\"raw_outputs/GBM/individual_GBMs/val/GBM_{i}_val.txt\", predictions_val)\n",
    "    write_predictions(f\"raw_outputs/GBM/individual_GBMs/test/GBM_{i}_test.txt\", predictions_test)\n",
    "\n",
    "    test_and_write(f\"test_output/GBM/GBM_{i}_val.csv\", predictions_val, val_queries)\n",
    "    test_and_write(f\"test_output/GBM/GBM_{i}_test.csv\", predictions_test, test_queries)\n",
    "\n",
    "    ensemble_predictions_val += predictions_val\n",
    "    ensemble_predictions_test += predictions_test\n",
    "\n",
    "# Average the predictions from the ensemble\n",
    "ensemble_predictions_val /= num_models\n",
    "ensemble_predictions_test /= num_models\n",
    "\n",
    "write_predictions(f\"raw_outputs/GBM/ensemble_GBMs/val/GBM_ensemble_val.txt\", ensemble_predictions_val)\n",
    "write_predictions(f\"raw_outputs/GBM/ensemble_GBMs/test/GBM_ensemble_test.txt\", ensemble_predictions_test)\n",
    "\n",
    "test_and_write(f\"test_output/GBM/GBM_ensemble_val.csv\", predictions_val, val_queries)\n",
    "test_and_write(f\"test_output/GBM/GBM_ensemble_test.csv\", predictions_test, test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:37:51.262051Z",
     "start_time": "2023-05-22T20:37:26.558609200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM model number 0 has a validation NDCG@5 score of 0.0\n",
      "GBM model number 1 has a validation NDCG@5 score of 0.0\n",
      "GBM model number 2 has a validation NDCG@5 score of 0.0\n",
      "GBM model number 3 has a validation NDCG@5 score of 0.0\n",
      "GBM model number 4 has a validation NDCG@5 score of 0.0\n",
      "The GBM ensemble model has a validation NDCG@5 score of 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate NDCG for validation set on all individual models and ensemble model\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    fn = f\"test_output/GBM/GBM_{i}_val.csv\"\n",
    "    NDGC_val_score = NDCG_from_file(fn, val_props2targets, 5)\n",
    "    print(f\"GBM model number {i} has a validation NDCG@5 score of {NDGC_val_score}\")\n",
    "\n",
    "\n",
    "fn = f\"test_output/GBM/GBM_ensemble_val.csv\"\n",
    "NDGC_val_score = NDCG_from_file(fn, val_props2targets, 5)\n",
    "print(f\"The GBM ensemble model has a validation NDCG@5 score of {NDGC_val_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-NN implementation as a stupid baseline, just because we needed to add something that's in the slides >:(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:02:09.517017700Z",
     "start_time": "2023-05-22T19:08:56.356382Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# if not os.path.isdir(\"test_output/kNN\"):\n",
    "#     os.makedirs(\"test_output/kNN\")\n",
    "\n",
    "# # Create a kNN classifier object\n",
    "# k = 5  # Number of neighbors to consider\n",
    "# knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# # Train the kNN classifier\n",
    "# t = time.time()\n",
    "# print(\"Training the k-NN model...\")\n",
    "# knn.fit(train_x, train_y)\n",
    "# print(f\"Finished training the k-NN model in {time.time() - t}s\")\n",
    "\n",
    "# # Predict the labels for the validation set\n",
    "# knn_pred_val = knn.predict(val_x)\n",
    "# knn_pred_test = knn.predict(test_x)\n",
    "\n",
    "# test_and_write(f\"test_output/kNN/kNN_val.csv\", knn_pred_val, val_queries)\n",
    "# test_and_write(f\"test_output/kNN/kNN_test.csv\", knn_pred_test, test_queries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is used for taking an ensemble of multiple ensembles. It uses the raw outputs saved to file earlier and takes the ensemble of these outputs to make a prediction and save that prediction to file. Of these predictions, the NDCG score is calculated.\n",
    "\n",
    "**Note: the ordering of the validation set is expected to be the same for all files. This means that the first element of all outputs is for the same search ID/property ID combination.** If this order is not the same, the ***BELOW CODE WILL NOT WORK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "100%|██████████| 194/194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ensemble of GBM ensemble models has a validation NDCG@5 score of 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# val_dir = \"raw_outputs/GBM/ensemble_GBMs/val/\"\n",
    "# test_dir = \"raw_outputs/GBM/ensemble_GBMs/test/\"\n",
    "\n",
    "# val_files = [f for f in os.listdir(val_dir) if os.path.isfile(os.path.join(val_dir, f))]\n",
    "# test_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n",
    "\n",
    "# num_ensembles = len(val_files)\n",
    "\n",
    "# ensemble_ensemble_val = np.zeros(len(val_x))\n",
    "# ensemble_ensemble_test = np.zeros(len(test_x))\n",
    "\n",
    "# # For the validation data\n",
    "# for i in range(num_ensembles):\n",
    "\n",
    "#     with open(os.path.join(val_dir, val_files[i])) as f_val:\n",
    "#         val_preds = np.array([float(line.strip(\"\\n\")) for line in f_val.readlines()])\n",
    "#         ensemble_ensemble_val += val_preds\n",
    "\n",
    "#     with open(os.path.join(test_dir, test_files[i])) as f_test:\n",
    "#         test_preds = np.array([float(line.strip(\"\\n\")) for line in f_test.readlines()])\n",
    "#         ensemble_ensemble_test += test_preds\n",
    "\n",
    "# ensemble_ensemble_val /= num_ensembles\n",
    "# ensemble_ensemble_test /= num_ensembles\n",
    "\n",
    "# # The importance of ordering is in this function: it expects the query IDs in val_queries to be of the same ordering\n",
    "# test_and_write(f\"test_output/GBM/GBM_big_ensemble_val.csv\", ensemble_ensemble_val, val_queries)\n",
    "# test_and_write(f\"test_output/GBM/GBM_big_ensemble_test.csv\", ensemble_ensemble_test, test_queries)\n",
    "\n",
    "\n",
    "# NDGC_val_score = NDCG_from_file(\"test_output/GBM/GBM_big_ensemble_val.csv\", val_props2targets, 5)\n",
    "# print(f\"The ensemble of GBM ensemble models has a validation NDCG@5 score of {NDGC_val_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
