{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:55.772067300Z",
     "start_time": "2023-05-22T18:30:55.761883300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:57.563630Z",
     "start_time": "2023-05-22T18:30:55.775424100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into pandas dataframes. The data has to be manually saved to a folder called 'data'.\n",
    "# Note: the data is quite large, so this may take a while (~40 seconds) if the data is read as a csv. To speed up further file reading, it is converted to pickle format.\n",
    "if (os.path.exists(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    & os.path.exists((os.path.join('data', 'test_set_VU_DM.pickle')))):\n",
    "    train_df = pd.read_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df = pd.read_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "else:\n",
    "    train_df = pd.read_csv(os.path.join('data', 'training_set_VU_DM.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('data', 'test_set_VU_DM.csv'))\n",
    "    train_df.to_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df.to_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "\n",
    "# train_df = pd.read_csv(os.path.join('data', 'dummy_training.csv'))\n",
    "# test_df = pd.read_csv(os.path.join('data', 'dummy_testing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.444183800Z",
     "start_time": "2023-05-22T18:30:57.596519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of data collection: 2012/11/01 - 2013/06/30\n",
      "Train data contains 4,958,347 rows and 54 columns\n",
      "Test data contains 4,959,183 rows and 50 columns\n",
      "\n",
      "Train data:\n",
      "Number of unique search IDs: 199,795\n",
      "Number of unique property IDs: 129,113\n",
      "Number of clicks per search: avg. 1.11, std. 0.21\n",
      "Number of bookings per search: avg. 0.69, std. 0.16\n",
      "\n",
      "Test data:\n",
      "Number of unique search IDs: 199,549\n",
      "Number of unique property IDs: 129,438\n"
     ]
    }
   ],
   "source": [
    "print(f\"Period of data collection: {pd.to_datetime(train_df['date_time']).min().strftime('%Y/%m/%d')} - {pd.to_datetime(train_df['date_time']).max().strftime('%Y/%m/%d')}\")\n",
    "print(f\"Train data contains {train_df.shape[0]:,} rows and {train_df.shape[1]} columns\")\n",
    "print(f\"Test data contains {test_df.shape[0]:,} rows and {test_df.shape[1]} columns\")\n",
    "print()\n",
    "print(f\"Train data:\")\n",
    "print(f\"Number of unique search IDs: {len(train_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(train_df['prop_id'].unique()):,}\")\n",
    "print(f\"Number of clicks per search: avg. {train_df['click_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['click_bool'].std():.2f}\")\n",
    "print(f\"Number of bookings per search: avg. {train_df['booking_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['booking_bool'].std():.2f}\")\n",
    "print()\n",
    "print(f\"Test data:\")\n",
    "print(f\"Number of unique search IDs: {len(test_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(test_df['prop_id'].unique()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.451457800Z",
     "start_time": "2023-05-22T18:30:59.450452700Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create the following new features: has_starrating, has_review_score, traveling_abroad, srch_prop_country_match, month, and day_of_week\n",
    "    \"\"\"\n",
    "    # has_starrating: boolean whether prop_starrating is 0 or null\n",
    "    df[\"has_starrating\"] = df[\"prop_starrating\"].isnull()\n",
    "    df[\"has_starrating\"] = df[\"has_starrating\"].astype(int)\n",
    "    df.loc[df[\"prop_starrating\"] == 0, \"has_starrating\"] = 1\n",
    "\n",
    "    # has_review_score: boolean whether prop_review_score is 0 or null\n",
    "    df[\"has_review_score\"] = df[\"prop_review_score\"].isnull()\n",
    "    df[\"has_review_score\"] = df[\"has_review_score\"].astype(int)\n",
    "    df.loc[df[\"prop_review_score\"] == 0, \"has_review_score\"] = 1\n",
    "\n",
    "    # traveling_abroad: boolean whether visitor_location_country_id != prop_country_id\n",
    "    df[\"traveling_abroad\"] = df[\"visitor_location_country_id\"] != df[\"prop_country_id\"]\n",
    "    df[\"traveling_abroad\"] = df[\"traveling_abroad\"].astype(int)\n",
    "\n",
    "    # srch_prop_country_match: boolean whether srch_destination_id == prop_country_id\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_destination_id\"] == df[\"prop_country_id\"]\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_prop_country_match\"].astype(int)\n",
    "\n",
    "    # month: month of the search, one-hot encoded\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"month\"] = df[\"date_time\"].dt.month\n",
    "    df[\"month\"] = df[\"month\"].map({1: \"jan\", 2: \"feb\", 3: \"mar\", 4: \"apr\", 5: \"may\", 6: \"jun\", 7: \"jul\", 8: \"aug\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dec\"})\n",
    "    df = pd.get_dummies(df, columns=[\"month\"], dtype=int)\n",
    "    for col in [\"month_jan\", \"month_feb\", \"month_mar\", \"month_apr\", \"month_may\", \"month_jun\", \"month_jul\", \"month_aug\", \"month_sep\", \"month_oct\", \"month_nov\", \"month_dec\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # day_of_week: day of the week of the search\n",
    "    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n",
    "    df[\"day_of_week\"] = df[\"day_of_week\"].map({0: \"mon\", 1: \"tue\", 2: \"wed\", 3: \"thu\", 4: \"fri\", 5: \"sat\", 6: \"sun\"})\n",
    "    df = pd.get_dummies(df, columns=[\"day_of_week\"], dtype=int)\n",
    "    for col in [\"day_of_week_mon\", \"day_of_week_tue\", \"day_of_week_wed\", \"day_of_week_thu\", \"day_of_week_fri\", \"day_of_week_sat\", \"day_of_week_sun\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.456697Z",
     "start_time": "2023-05-22T18:30:59.455690200Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_nan_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\", and \"compx_rate_percent_diff\" have >90% NaN values. These values cannot be imputed accurately, so we drop these columns.\n",
    "    \"\"\"\n",
    "    cols = [\"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\"] + [f\"comp{i}_rate_percent_diff\" for i in range(1, 9)]\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.463348700Z",
     "start_time": "2023-05-22T18:30:59.462342100Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values for the following columns: \"prop_starrating\", \"prop_review_score\", \"compx_rate\", and \"compx_inv\".\n",
    "\n",
    "    For \"prop_starrating\" and \"prop_review_score\", we replace 0 values with NaN and then impute the NaN values with the mean per srch_id. Remaining NaN values are then filled with 0.\n",
    "    For \"compx_rate\" and \"compx_inv\", we assume that missing data means that Expedia has the same price and equal availability as its competitors. We therefore impute the NaN values with 0.\n",
    "    \"\"\"\n",
    "    # Replace 0 values with NaN\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].replace(0, np.nan)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].replace(0, np.nan)\n",
    "    # Impute NaN values with mean per srch_id\n",
    "    df[\"prop_starrating\"] = df.groupby(\"srch_id\")[\"prop_starrating\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    df[\"prop_review_score\"] = df.groupby(\"srch_id\")[\"prop_review_score\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    # Fill remaining NaN values with 0\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].fillna(0)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].fillna(0)\n",
    "\n",
    "    # Impute NaN values with 0\n",
    "    for i in range(1, 9):\n",
    "        df[f\"comp{i}_rate\"] = df[f\"comp{i}_rate\"].fillna(0)\n",
    "        df[f\"comp{i}_inv\"] = df[f\"comp{i}_inv\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.470380900Z",
     "start_time": "2023-05-22T18:30:59.468366600Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_aggregated_values(df):\n",
    "    \"\"\"\n",
    "    Compute the mean, median and standard deviation for the following columns:\n",
    "    \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "    \"\"\"\n",
    "    numerical_cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    # srch_length_of_stay, srch_booking_window, srch_adults_count, srch_children_count, and srch_room_count are also numerical variables, but it has no use aggregating these values over prop_id.\n",
    "\n",
    "    agg_df = df.groupby(\"prop_id\").agg({col: [\"mean\", \"std\", \"median\"] for col in numerical_cols})\n",
    "    agg_df.columns = [\"_\".join(col) for col in agg_df.columns]\n",
    "    agg_df.fillna(0, inplace=True)  # Fill standard deviation NaN values with 0\n",
    "    for col in agg_df.columns:\n",
    "        df[col] = df[\"prop_id\"].map(agg_df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.497541600Z",
     "start_time": "2023-05-22T18:30:59.473404100Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_relative_values(df):\n",
    "    \"\"\"\n",
    "    Subtract the mean per srch_id from the following columns and make them into new columns:\n",
    "    \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "\n",
    "    This is done so that the model can learn the relative values of these columns per srch_id.\n",
    "    \"\"\"\n",
    "    cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    grouper = df.groupby('srch_id')\n",
    "    for col in cols:\n",
    "        df[f\"relative_{col}\"] = df[col] - grouper[col].transform('mean')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.477989600Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that for \"orig_destination_distance\" over 75% of the data with a calculated value lower than 0.95 of the largest distance was lower than 130, meaning that the distance per srch_id is roughly the same. We assume therefore that this is not a deciding factor for a customer in their booking process and drop this column.\n",
    "\n",
    "    Features were created from \"date_time\" and the column will not be used anymore, so we drop this column as well.\n",
    "\n",
    "    Columns containing IDs (\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"prop_id\", and \"srch_destination_id\") are not used in the model, so we drop these columns as well. \"srch_id\" and \"prop_id\" will remain in the columns for now for later use.\n",
    "\n",
    "    If the supplied dataframe is the training dataframe, drop the unused target columns as well.\n",
    "\n",
    "    # TODO:\n",
    "    I don't really know what to do with \"prop_location_score2\" yet, so I'll drop it for now.\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"orig_destination_distance\"], inplace=True)\n",
    "    df.drop(columns=[\"date_time\"], inplace=True)\n",
    "    df.drop(columns=[\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"srch_destination_id\"], inplace=True)\n",
    "    df.drop(columns=[\"prop_location_score2\"], inplace=True)\n",
    "    for col in [\"position\", \"gross_bookings_usd\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.483040900Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(train_df: pd.DataFrame, test_df: pd.DataFrame, target_value: str = \"booking_bool\", train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Process the dataframes for training and testing the model.\n",
    "    :param train_df: The training dataframe\n",
    "    :param test_df: The test dataframe\n",
    "    :param target_value: The target value to use for training the model. Either \"booking_bool\" or \"both\".\n",
    "    :return: The processed dataframes\n",
    "    \"\"\"\n",
    "    # Create features\n",
    "    train_df = create_features(train_df)\n",
    "    test_df = create_features(test_df)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    train_df = drop_nan_columns(train_df)\n",
    "    test_df = drop_nan_columns(test_df)\n",
    "\n",
    "    # Impute missing values\n",
    "    train_df = impute_missing_values(train_df)\n",
    "    test_df = impute_missing_values(test_df)\n",
    "\n",
    "    # Compute aggregated values\n",
    "    train_df = compute_aggregated_values(train_df)\n",
    "    test_df = compute_aggregated_values(test_df)\n",
    "\n",
    "    # Compute relative values\n",
    "    train_df = compute_relative_values(train_df)\n",
    "    test_df = compute_relative_values(test_df)\n",
    "\n",
    "    # Drop columns\n",
    "    train_df = drop_columns(train_df)\n",
    "    test_df = drop_columns(test_df)\n",
    "\n",
    "    # Split train data into features and target\n",
    "    if target_value == \"booking_bool\":\n",
    "        train_df[\"target\"] = train_df[\"booking_bool\"]\n",
    "    elif target_value == \"both\":\n",
    "        train_df[\"target\"] = 5*train_df[\"booking_bool\"] + train_df[\"click_bool\"]\n",
    "    else:\n",
    "        raise ValueError(\"target_value must be either 'booking_bool' or 'both'\")\n",
    "\n",
    "    train_df.drop(columns=[\"booking_bool\", \"click_bool\"], inplace=True)\n",
    "\n",
    "    train_df = train_df[:int(train_frac * len(train_df))]\n",
    "    val_df = train_df[int(train_frac * len(train_df)):]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.498046900Z",
     "start_time": "2023-05-22T18:30:59.496014200Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences(df: pd.DataFrame, kind: str = \"train\") -> dict:\n",
    "    \"\"\"\n",
    "    Generate sequences from the dataframe.\n",
    "    :param df: The dataframe to take the sequences from.\n",
    "    :param kind: The kind of dataframe. Either \"train\", \"val\" or \"test\".\n",
    "    :return: Dictionary containing the sequences and targets for the training data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    if kind == \"train\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\", \"target\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Train DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values, \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"val\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\", \"target\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Validation DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values, \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"test\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Test DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values})\n",
    "    else:\n",
    "        raise ValueError(\"kind must be either 'train', 'val' or 'test'\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:59.512486500Z",
     "start_time": "2023-05-22T18:30:59.508476500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_lgb_sequences(train_df, test_df, force_data_processing=False, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Generate the train, validation and test dictionaries\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join('data', f'train_sequences_both.pickle')) \\\n",
    "            & os.path.exists(os.path.join('data', f'test_sequences_both.pickle')) \\\n",
    "            & os.path.exists(os.path.join('data', f'val_sequences_both.pickle')) \\\n",
    "            & (not force_data_processing):\n",
    "        print(f\"Loading train and test sequences from pickle files...\")\n",
    "        with open(os.path.join('data', f'train_sequences_both.pickle'), 'rb') as f:\n",
    "            train_sequences = pickle.load(f)\n",
    "        with open(os.path.join('data', f'val_sequences_both.pickle'), 'rb') as f:\n",
    "            val_sequences = pickle.load(f)\n",
    "        with open(os.path.join('data', f'test_sequences_both.pickle'), 'rb') as f:\n",
    "            test_sequences = pickle.load(f)\n",
    "    else:\n",
    "        processed_train_df, processed_val_df, processed_test_df = process_data(train_df, test_df, target_value=\"both\", train_frac=train_frac)\n",
    "        train_sequences = generate_sequences(processed_train_df, kind=\"train\")\n",
    "        with open(os.path.join('data', f'train_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(train_sequences, f)\n",
    "        val_sequences = generate_sequences(processed_val_df, kind=\"val\")\n",
    "        with open(os.path.join('data', f'val_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(val_sequences, f)    \n",
    "        test_sequences = generate_sequences(processed_test_df, kind=\"test\")\n",
    "        with open(os.path.join('data', f'test_sequences_both.pickle'), 'wb') as f:\n",
    "            pickle.dump(test_sequences, f)\n",
    "\n",
    "    return train_sequences, val_sequences, test_sequences\n",
    "\n",
    "def generate_lgb_data(sequences, kind=\"train\"):\n",
    "    x_data = np.array([seq[\"sequence\"] for seq in sequences])\n",
    "    \n",
    "    query_to_properties = {}\n",
    "    query_to_properties_to_target = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        \n",
    "        srch_id = seq[\"srch_id\"]\n",
    "        prop_id = seq[\"prop_id\"]\n",
    "        # Add the search ID to the dictionary if it isn't in already\n",
    "        if srch_id not in query_to_properties.keys():\n",
    "            query_to_properties[srch_id] = []\n",
    "            query_to_properties_to_target[srch_id] = {}\n",
    "        \n",
    "        query_to_properties[srch_id].append(prop_id)\n",
    "        \n",
    "        if kind ==\"train\" or kind==\"val\":\n",
    "            query_to_properties_to_target[srch_id][prop_id] = seq[\"target\"]\n",
    "        \n",
    "    # When generating train data, return also targets\n",
    "    if kind==\"train\" or kind==\"val\":\n",
    "        y_data = np.array([seq[\"target\"] for seq in sequences])\n",
    "\n",
    "        return x_data, y_data, query_to_properties, query_to_properties_to_target\n",
    "\n",
    "    # When generating test data, return only features and query information\n",
    "    elif kind==\"test\":\n",
    "\n",
    "        return x_data, query_to_properties\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset creation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the LambdaMART model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:22.803150600Z",
     "start_time": "2023-05-22T18:30:59.514486500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train DataLoader: 100%|██████████| 4462512/4462512 [09:10<00:00, 8108.94it/s]\n",
      "Validation DataLoader: 100%|██████████| 446252/446252 [00:50<00:00, 8915.22it/s]\n",
      "Test DataLoader: 100%|██████████| 4959183/4959183 [10:29<00:00, 7881.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating sequences. This took 1866.71s\n",
      "Generating train features...\n",
      "Done generating train features. This took 71.91s\n",
      "Generating validation features...\n",
      "Done generating validation features. This took 3.04s\n",
      "Generating test features...\n",
      "Done generating test features. This took 60.69s\n",
      "Transforming train features into appropriate dataset...\n",
      "Done generating dataset. This took 0.35s\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "train_frac = 0.9\n",
    "\n",
    "# Assume X_train and y_train are the training features and target values, respectively\n",
    "print(\"Generating sequences...\")\n",
    "t = time.time()\n",
    "lgb_train_sequences, lgb_val_sequences, lgb_test_sequences = generate_lgb_sequences(train_df, test_df, force_data_processing=False, train_frac=train_frac)\n",
    "print(f\"Done generating sequences. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating train features...\")\n",
    "train_x, train_y, train_queries, train_props2targets = generate_lgb_data(lgb_train_sequences, kind=\"train\")\n",
    "print(f\"Done generating train features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating validation features...\")\n",
    "val_x, val_y, val_queries, val_props2targets = generate_lgb_data(lgb_val_sequences, kind=\"val\")\n",
    "print(f\"Done generating validation features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Generating test features...\")\n",
    "test_x, test_queries = generate_lgb_data(lgb_test_sequences, kind=\"test\")\n",
    "print(f\"Done generating test features. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "print(\"Transforming train features into appropriate dataset...\")\n",
    "# Create a LightGBM dataset from the training data\n",
    "train_data = lgb.Dataset(train_x, label=train_y, group=[len(elem) for elem in train_queries.values()])\n",
    "print(f\"Done generating dataset. This took {time.time() - t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:22.999267400Z",
     "start_time": "2023-05-22T19:04:22.922215900Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_and_write(fn, predictions, queries):\n",
    "    \"\"\"\n",
    "    Function to write a model's predictions to disk, where queries is a dictionary mapping\n",
    "    a query ID to a list of property IDs, and predictions is an (ordered!) list containing\n",
    "    the output of the model. \n",
    "    \"\"\"\n",
    "    srch_to_ranks = {}\n",
    "    \n",
    "    i=0\n",
    "    for query_id, property_ids in tqdm(queries.items()):\n",
    "        \n",
    "        # Retrieve all probabilities for a certain query ID (assuming the data is still ordered)\n",
    "        ranks = predictions[i:i+len(property_ids)]\n",
    "\n",
    "        # Keep track of the probabilities\n",
    "        srch_to_ranks[query_id] = ranks\n",
    "        i+=len(property_ids)\n",
    "    \n",
    "    # Write predictions to file\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(\"srch_id,prop_id\\n\")\n",
    "\n",
    "        # Iterate over all different queries\n",
    "        for srch_id, srch_probs in srch_to_ranks.items():\n",
    "            \n",
    "            # Get the order of the hotels based on the predicted probabilities\n",
    "            sorted_indices = np.array(srch_probs).argsort()[::-1]\n",
    "            property_ids_sorted = np.array(queries[srch_id])[np.array(sorted_indices)]\n",
    "\n",
    "            for s_id, p_id in zip([srch_id for _ in range(len(property_ids_sorted))],property_ids_sorted):\n",
    "                f.write(f\"{s_id},{p_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:37:21.306745600Z",
     "start_time": "2023-05-22T20:37:21.302218900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate NDCG@k score from file\n",
    "\n",
    "def NDCG_from_file(fn, prop2targets, k):\n",
    "    \"\"\"\n",
    "    Function to calculate the NDCG@k score from a txt file in the format specified in the assignment (two \n",
    "    columns, the left one containing the search IDs and the right one containing the PropertyIDs, sorted by\n",
    "    relevance)\n",
    "    \"\"\"\n",
    "    query_to_properties = {}\n",
    "\n",
    "    # Fill the query-to-properties dictionary\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()[1:] # Remove the header\n",
    "        for line in lines:\n",
    "            query_id, prop_id = line.split(\",\")\n",
    "            query_id, prop_id = int(query_id), int(prop_id)\n",
    "            if query_id not in query_to_properties.keys():\n",
    "                query_to_properties[query_id] = [prop_id]\n",
    "            else:\n",
    "                query_to_properties[query_id].append(prop_id)\n",
    "\n",
    "    NDCG_score = 0\n",
    "    for query_id, prop_ids in query_to_properties.items():\n",
    "        # Get all scores in order how they are scored by the model\n",
    "        all_scores = np.array([prop2targets[query_id][prop_id] for prop_id in prop_ids])\n",
    "\n",
    "        # Take only the first k elements\n",
    "        true_scores = all_scores[:k]\n",
    "\n",
    "        # Calculate DCG@k (Discounted Cumulative Gain at k)\n",
    "        dcg = np.sum(true_scores / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Sort the true scores in descending order\n",
    "        true_sorted_scores = np.sort(all_scores)[:k][::-1]\n",
    "        \n",
    "        # Calculate ideal DCG@k\n",
    "        ideal_dcg = np.sum(true_sorted_scores[:k] / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Calculate NDCG@k\n",
    "        NDCG_score += (dcg / ideal_dcg) if ideal_dcg > 0 else 0.0\n",
    "\n",
    "    return NDCG_score / len(query_to_properties.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.003856900Z",
     "start_time": "2023-05-22T19:04:22.942120800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the parameters for the LambdaMART model\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],  # Evaluation at NDCG@5\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'device': 'gpu' # Change to GPU if available\n",
    "}\n",
    "\n",
    "# TODO: check what hyperparameters winners used and optimize\n",
    "gbm_params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "num_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.003856900Z",
     "start_time": "2023-05-22T19:04:22.943117200Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train the LambdaMART model\n",
    "\n",
    "# t = time.time()\n",
    "# print(\"Training LambdaMART model...\")\n",
    "# vanilla_mart_model = lgb.train(params, train_data, num_rounds)\n",
    "# print(f\"Done training LambdaMART model. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "# # Save the trained model\n",
    "# vanilla_mart_model.save_model('lambda_mart_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:04:23.390592100Z",
     "start_time": "2023-05-22T19:04:22.943117200Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Training GBM model...\")\n",
    "# GBM_model = lgb.train(gbm_params, train_data, num_rounds)\n",
    "# print(f\"Done training GBM model. This took {time.time() - t:.2f}s\")\n",
    "\n",
    "# # Save the trained model\n",
    "# GBM_model.save_model('gbm_model.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training of the ensemble model**\n",
    "\n",
    "This one should work the best but I kept the code above just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T19:08:44.712089Z",
     "start_time": "2023-05-22T19:04:22.948707800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble of GBM models\n",
      "Training model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niels\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Training model 2\n",
      "Training model 3\n",
      "Training model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18026/18026 [00:00<00:00, 21606.40it/s]\n",
      "100%|██████████| 199549/199549 [00:06<00:00, 31261.20it/s]\n",
      "100%|██████████| 18026/18026 [00:00<00:00, 759132.13it/s]\n",
      "100%|██████████| 199549/199549 [00:00<00:00, 1450947.95it/s]\n",
      "100%|██████████| 18026/18026 [00:00<00:00, 941515.56it/s]\n",
      "100%|██████████| 199549/199549 [00:00<00:00, 1522589.86it/s]\n",
      "100%|██████████| 18026/18026 [00:00<00:00, 1282509.90it/s]\n",
      "100%|██████████| 199549/199549 [00:00<00:00, 1551681.36it/s]\n",
      "100%|██████████| 18026/18026 [00:00<00:00, 1324919.37it/s]\n",
      "100%|██████████| 199549/199549 [00:00<00:00, 1551713.01it/s]\n",
      "100%|██████████| 18026/18026 [00:00<00:00, 1313616.72it/s]\n",
      "100%|██████████| 199549/199549 [00:00<00:00, 1516023.29it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"test_output/GBM\"):\n",
    "    os.makedirs(\"test_output/GBM\")\n",
    "\n",
    "# Define the number of models in the ensemble\n",
    "num_models = 5\n",
    "\n",
    "# Train the ensemble of GBM models\n",
    "models = []\n",
    "print(f\"Training ensemble of GBM models\")\n",
    "for i in range(num_models):\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(42 + i)\n",
    "    \n",
    "    # Create a random subsample of the training data\n",
    "    subsample_indices = np.random.choice(len(train_x), size=len(train_x), replace=True)\n",
    "    subsample_data = train_data.subset(subsample_indices)\n",
    "    \n",
    "    # Train an individual GBM model\n",
    "    print(f\"Training model {i}\")\n",
    "    model = lgb.train(gbm_params, subsample_data, num_boost_round=100)\n",
    "    \n",
    "    # Add the trained model to the ensemble\n",
    "    models.append(model)\n",
    "\n",
    "# Make predictions using the ensemble of models\n",
    "ensemble_predictions_val = np.zeros(len(val_x))\n",
    "ensemble_predictions_test = np.zeros(len(test_x))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    predictions_val = model.predict(val_x)\n",
    "    predictions_test = model.predict(test_x)\n",
    "\n",
    "    test_and_write(f\"test_output/GBM/GBM_{i}_val.csv\", predictions_val, val_queries)\n",
    "    test_and_write(f\"test_output/GBM/GBM_{i}_test.csv\", predictions_test, test_queries)\n",
    "\n",
    "    ensemble_predictions_val += predictions_val\n",
    "    ensemble_predictions_test += predictions_test\n",
    "\n",
    "# Average the predictions from the ensemble\n",
    "ensemble_predictions_val /= num_models\n",
    "ensemble_predictions_test /= num_models\n",
    "\n",
    "test_and_write(f\"test_output/GBM/GBM_ensemble_val.csv\", predictions_val, val_queries)\n",
    "test_and_write(f\"test_output/GBM/GBM_ensemble_test.csv\", predictions_test, test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:37:51.262051Z",
     "start_time": "2023-05-22T20:37:26.558609200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM model number 0 has a validation NDCG@5 score of 0.01906336173976899\n",
      "GBM model number 1 has a validation NDCG@5 score of 0.019303880565172276\n",
      "GBM model number 2 has a validation NDCG@5 score of 0.019087860280537328\n",
      "GBM model number 3 has a validation NDCG@5 score of 0.0191777761769073\n",
      "GBM model number 4 has a validation NDCG@5 score of 0.019283069929021956\n",
      "The GBM ensemble model has a validation NDCG@5 score of 0.019283069929021956\n"
     ]
    }
   ],
   "source": [
    "# Calculate NDCG for validation set on all individual models and ensemble model\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    fn = f\"test_output/GBM/GBM_{i}_val.csv\"\n",
    "    NDGC_val_score = NDCG_from_file(fn, val_props2targets, 5)\n",
    "    print(f\"GBM model number {i} has a validation NDCG@5 score of {NDGC_val_score}\")\n",
    "\n",
    "\n",
    "fn = f\"test_output/GBM/GBM_ensemble_val.csv\"\n",
    "NDGC_val_score = NDCG_from_file(fn, val_props2targets, 5)\n",
    "print(f\"The GBM ensemble model has a validation NDCG@5 score of {NDGC_val_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-NN implementation as a stupid baseline, just because we needed to add something that's in the slides >:(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T20:02:09.517017700Z",
     "start_time": "2023-05-22T19:08:56.356382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the k-NN model...\n",
      "Finished training the k-NN model in 10.506479740142822s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training the k-NN model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Predict the labels for the validation set\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m knn_pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m knn_pred_test \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(test_x)\n\u001b[0;32m     20\u001b[0m test_and_write(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_output/kNN/kNN_val.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, knn_pred_val, val_queries)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:824\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    817\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    820\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    821\u001b[0m     )\n\u001b[0;32m    822\u001b[0m )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 824\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    836\u001b[0m ):\n\u001b[0;32m    837\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    838\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    839\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:277\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    290\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    291\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    298\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:95\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "if not os.path.isdir(\"test_output/kNN\"):\n",
    "    os.makedirs(\"test_output/kNN\")\n",
    "\n",
    "# Create a kNN classifier object\n",
    "k = 5  # Number of neighbors to consider\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the kNN classifier\n",
    "t = time.time()\n",
    "print(\"Training the k-NN model...\")\n",
    "knn.fit(train_x, train_y)\n",
    "print(f\"Finished training the k-NN model in {time.time() - t}s\")\n",
    "\n",
    "# Predict the labels for the validation set\n",
    "knn_pred_val = knn.predict(val_x)\n",
    "knn_pred_test = knn.predict(test_x)\n",
    "\n",
    "test_and_write(f\"test_output/kNN/kNN_val.csv\", knn_pred_val, val_queries)\n",
    "test_and_write(f\"test_output/kNN/kNN_test.csv\", knn_pred_test, test_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
