{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyproxius\\anaconda3\\envs\\dmt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas dataframes. The data has to be manually saved to a folder called 'data'.\n",
    "# Note: the data is quite large, so this may take a while (~40 seconds) if the data is read as a csv. To speed up further file reading, it is converted to pickle format.\n",
    "if (os.path.exists(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    & os.path.exists((os.path.join('data', 'test_set_VU_DM.pickle')))):\n",
    "    train_df = pd.read_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df = pd.read_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "else:\n",
    "    train_df = pd.read_csv(os.path.join('data', 'training_set_VU_DM.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('data', 'test_set_VU_DM.csv'))\n",
    "    train_df.to_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df.to_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "\n",
    "# train_df = pd.read_csv(os.path.join('data', 'dummy_training.csv'))\n",
    "# test_df = pd.read_csv(os.path.join('data', 'dummy_testing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of data collection: 2012/11/01 - 2013/06/29\n",
      "Train data contains 4,999 rows and 54 columns\n",
      "Test data contains 4,999 rows and 50 columns\n",
      "\n",
      "Train data:\n",
      "Number of unique search IDs: 207\n",
      "Number of unique property IDs: 4,602\n",
      "Number of clicks per search: avg. 1.09, std. 0.21\n",
      "Number of bookings per search: avg. 0.66, std. 0.16\n",
      "\n",
      "Test data:\n",
      "Number of unique search IDs: 194\n",
      "Number of unique property IDs: 4,551\n"
     ]
    }
   ],
   "source": [
    "print(f\"Period of data collection: {pd.to_datetime(train_df['date_time']).min().strftime('%Y/%m/%d')} - {pd.to_datetime(train_df['date_time']).max().strftime('%Y/%m/%d')}\")\n",
    "print(f\"Train data contains {train_df.shape[0]:,} rows and {train_df.shape[1]} columns\")\n",
    "print(f\"Test data contains {test_df.shape[0]:,} rows and {test_df.shape[1]} columns\")\n",
    "print()\n",
    "print(f\"Train data:\")\n",
    "print(f\"Number of unique search IDs: {len(train_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(train_df['prop_id'].unique()):,}\")\n",
    "print(f\"Number of clicks per search: avg. {train_df['click_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['click_bool'].std():.2f}\")\n",
    "print(f\"Number of bookings per search: avg. {train_df['booking_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['booking_bool'].std():.2f}\")\n",
    "print()\n",
    "print(f\"Test data:\")\n",
    "print(f\"Number of unique search IDs: {len(test_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(test_df['prop_id'].unique()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create the following new features: has_starrating, has_review_score, traveling_abroad, srch_prop_country_match, month, and day_of_week\n",
    "    \"\"\"\n",
    "    # has_starrating: boolean whether prop_starrating is 0 or null\n",
    "    df[\"has_starrating\"] = df[\"prop_starrating\"].isnull()\n",
    "    df[\"has_starrating\"] = df[\"has_starrating\"].astype(int)\n",
    "    df.loc[df[\"prop_starrating\"] == 0, \"has_starrating\"] = 1\n",
    "\n",
    "    # has_review_score: boolean whether prop_review_score is 0 or null\n",
    "    df[\"has_review_score\"] = df[\"prop_review_score\"].isnull()\n",
    "    df[\"has_review_score\"] = df[\"has_review_score\"].astype(int)\n",
    "    df.loc[df[\"prop_review_score\"] == 0, \"has_review_score\"] = 1\n",
    "\n",
    "    # traveling_abroad: boolean whether visitor_location_country_id != prop_country_id\n",
    "    df[\"traveling_abroad\"] = df[\"visitor_location_country_id\"] != df[\"prop_country_id\"]\n",
    "    df[\"traveling_abroad\"] = df[\"traveling_abroad\"].astype(int)\n",
    "\n",
    "    # srch_prop_country_match: boolean whether srch_destination_id == prop_country_id\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_destination_id\"] == df[\"prop_country_id\"]\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_prop_country_match\"].astype(int)\n",
    "\n",
    "    # month: month of the search, one-hot encoded\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"month\"] = df[\"date_time\"].dt.month\n",
    "    df[\"month\"] = df[\"month\"].map({1: \"jan\", 2: \"feb\", 3: \"mar\", 4: \"apr\", 5: \"may\", 6: \"jun\", 7: \"jul\", 8: \"aug\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dec\"})\n",
    "    df = pd.get_dummies(df, columns=[\"month\"], dtype=int)\n",
    "    for col in [\"month_jan\", \"month_feb\", \"month_mar\", \"month_apr\", \"month_may\", \"month_jun\", \"month_jul\", \"month_aug\", \"month_sep\", \"month_oct\", \"month_nov\", \"month_dec\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # day_of_week: day of the week of the search\n",
    "    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n",
    "    df[\"day_of_week\"] = df[\"day_of_week\"].map({0: \"mon\", 1: \"tue\", 2: \"wed\", 3: \"thu\", 4: \"fri\", 5: \"sat\", 6: \"sun\"})\n",
    "    df = pd.get_dummies(df, columns=[\"day_of_week\"], dtype=int)\n",
    "    for col in [\"day_of_week_mon\", \"day_of_week_tue\", \"day_of_week_wed\", \"day_of_week_thu\", \"day_of_week_fri\", \"day_of_week_sat\", \"day_of_week_sun\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\", and \"compx_rate_percent_diff\" have >90% NaN values. These values cannot be imputed accurately, so we drop these columns.\n",
    "    \"\"\"\n",
    "    cols = [\"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\"] + [f\"comp{i}_rate_percent_diff\" for i in range(1, 9)]\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values for the following columns: \"prop_starrating\", \"prop_review_score\", \"compx_rate\", and \"compx_inv\".\n",
    "\n",
    "    For \"prop_starrating\" and \"prop_review_score\", we replace 0 values with NaN and then impute the NaN values with the mean per srch_id. Remaining NaN values are then filled with 0.\n",
    "    For \"compx_rate\" and \"compx_inv\", we assume that missing data means that Expedia has the same price and equal availability as its competitors. We therefore impute the NaN values with 0.\n",
    "    \"\"\"\n",
    "    # Replace 0 values with NaN\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].replace(0, np.nan)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].replace(0, np.nan)\n",
    "    # Impute NaN values with mean per srch_id\n",
    "    df[\"prop_starrating\"] = df.groupby(\"srch_id\")[\"prop_starrating\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    df[\"prop_review_score\"] = df.groupby(\"srch_id\")[\"prop_review_score\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    # Fill remaining NaN values with 0\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].fillna(0)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].fillna(0)\n",
    "\n",
    "    # Impute NaN values with 0\n",
    "    for i in range(1, 9):\n",
    "        df[f\"comp{i}_rate\"] = df[f\"comp{i}_rate\"].fillna(0)\n",
    "        df[f\"comp{i}_inv\"] = df[f\"comp{i}_inv\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_values(df):\n",
    "    \"\"\"\n",
    "    Compute the mean, median and standard deviation for the following columns:\n",
    "    \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "    \"\"\"\n",
    "    numerical_cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    # srch_length_of_stay, srch_booking_window, srch_adults_count, srch_children_count, and srch_room_count are also numerical variables, but it has no use aggregating these values over prop_id.\n",
    "\n",
    "    agg_df = df.groupby(\"prop_id\").agg({col: [\"mean\", \"std\", \"median\"] for col in numerical_cols})\n",
    "    agg_df.columns = [\"_\".join(col) for col in agg_df.columns]\n",
    "    agg_df.fillna(0, inplace=True)  # Fill standard deviation NaN values with 0\n",
    "    for col in agg_df.columns:\n",
    "        df[col] = df[\"prop_id\"].map(agg_df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_values(df):\n",
    "    \"\"\"\n",
    "    Subtract the mean per srch_id from the following columns:\n",
    "    \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "\n",
    "    This is done so that the model can learn the relative values of these columns per srch_id.\n",
    "    \"\"\"\n",
    "    cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    grouper = df.groupby('srch_id')\n",
    "    for col in cols:\n",
    "        df[col] = df[col] - grouper[col].transform('mean')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that for \"orig_destination_distance\" over 75% of the data with a calculated value lower than 0.95 of the largest distance was lower than 130, meaning that the distance per srch_id is roughly the same. We assume therefore that this is not a deciding factor for a customer in their booking process and drop this column.\n",
    "\n",
    "    Features were created from \"date_time\" and the column will not be used anymore, so we drop this column as well.\n",
    "\n",
    "    Columns containing IDs (\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"prop_id\", and \"srch_destination_id\") are not used in the model, so we drop these columns as well. \"srch_id\" and \"prop_id\" will remain in the columns for now for later use.\n",
    "\n",
    "    If the supplied dataframe is the training dataframe, drop the unused target columns as well.\n",
    "\n",
    "    # TODO:\n",
    "    I don't really know what to do with \"prop_location_score2\" yet, so I'll drop it for now.\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"orig_destination_distance\"], inplace=True)\n",
    "    df.drop(columns=[\"date_time\"], inplace=True)\n",
    "    df.drop(columns=[\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"srch_destination_id\"], inplace=True)\n",
    "    df.drop(columns=[\"prop_location_score2\"], inplace=True)\n",
    "    for col in [\"position\", \"gross_bookings_usd\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_df: pd.DataFrame, test_df: pd.DataFrame, target_value: str = \"booking_bool\"):\n",
    "    \"\"\"\n",
    "    Process the dataframes for training and testing the model.\n",
    "    :param train_df: The training dataframe\n",
    "    :param test_df: The test dataframe\n",
    "    :param target_value: The target value to use for training the model. Either \"booking_bool\" or \"both\".\n",
    "    :return: The processed dataframes\n",
    "    \"\"\"\n",
    "    # Create features\n",
    "    train_df = create_features(train_df)\n",
    "    test_df = create_features(test_df)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    train_df = drop_nan_columns(train_df)\n",
    "    test_df = drop_nan_columns(test_df)\n",
    "\n",
    "    # Impute missing values\n",
    "    train_df = impute_missing_values(train_df)\n",
    "    test_df = impute_missing_values(test_df)\n",
    "\n",
    "    # Compute aggregated values\n",
    "    train_df = compute_aggregated_values(train_df)\n",
    "    test_df = compute_aggregated_values(test_df)\n",
    "\n",
    "    # Compute relative values\n",
    "    train_df = compute_relative_values(train_df)\n",
    "    test_df = compute_relative_values(test_df)\n",
    "\n",
    "    # Drop columns\n",
    "    train_df = drop_columns(train_df)\n",
    "    test_df = drop_columns(test_df)\n",
    "\n",
    "    # Split train data into features and target\n",
    "    if target_value == \"booking_bool\":\n",
    "        train_df[\"target\"] = train_df[\"booking_bool\"]\n",
    "    elif target_value == \"both\":\n",
    "        train_df[\"target\"] = 5*train_df[\"booking_bool\"] + train_df[\"click_bool\"]\n",
    "    else:\n",
    "        raise ValueError(\"target_value must be either 'booking_bool' or 'both'\")\n",
    "\n",
    "    train_df.drop(columns=[\"booking_bool\", \"click_bool\"], inplace=True)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(df: pd.DataFrame, kind: str = \"train\") -> dict:\n",
    "    \"\"\"\n",
    "    Generate sequences from the dataframe.\n",
    "    :param df: The dataframe to take the sequences from.\n",
    "    :param kind: The kind of dataframe. Either \"train\" or \"test\".\n",
    "    :return: Dictionary containing the sequences and targets for the training data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    if kind == \"train\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\", \"target\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Train DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values, \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"test\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df = df.drop(columns=[\"prop_id\", \"srch_id\"])\n",
    "        for i, idx in enumerate(tqdm(idx_dict, desc=\"Train DataLoader\")):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df.iloc[i].values})\n",
    "    else:\n",
    "        raise ValueError(\"kind must be either 'train' or 'test'\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lgb_sequences(train_df, test_df):\n",
    "\n",
    "    processed_train_df, processed_test_df = process_data(train_df, test_df, target_value=\"both\")\n",
    "\n",
    "    train_sequences = generate_sequences(processed_train_df, kind=\"train\")\n",
    "    test_sequences = generate_sequences(processed_test_df, kind=\"test\")\n",
    "\n",
    "    return train_sequences, test_sequences\n",
    "\n",
    "def generate_lgb_data(sequences, kind=\"train\"):\n",
    "    train_x = np.array([seq[\"sequence\"] for seq in sequences])\n",
    "    \n",
    "    query_to_properties = {}\n",
    "    for seq in sequences:\n",
    "        \n",
    "        srch_id = seq[\"srch_id\"]\n",
    "        prop_id = seq[\"prop_id\"]\n",
    "        # Add the search ID to the dictionary if it isn't in already\n",
    "        if srch_id not in query_to_properties.keys():\n",
    "            query_to_properties[srch_id] = []\n",
    "        \n",
    "        query_to_properties[srch_id].append(prop_id)\n",
    "\n",
    "    # When generating train data, return also targets\n",
    "    if kind==\"train\":\n",
    "        train_y = np.array([seq[\"target\"] for seq in sequences])\n",
    "\n",
    "        return train_x, train_y, query_to_properties\n",
    "    \n",
    "    # When generating test data, return only features and query information\n",
    "    elif kind==\"test\":\n",
    "\n",
    "        return train_x, query_to_properties\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset creation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the LambdaMART model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train DataLoader: 100%|██████████| 4999/4999 [00:00<00:00, 7521.77it/s]\n",
      "Train DataLoader: 100%|██████████| 4999/4999 [00:00<00:00, 7648.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x11a355de470>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Assume X_train and y_train are the training features and target values, respectively\n",
    "lgb_train_sequences, lgb_test_sequences = generate_lgb_sequences(train_df, test_df)\n",
    "\n",
    "train_x, train_y, train_queries = generate_lgb_data(lgb_train_sequences, kind=\"train\")\n",
    "\n",
    "# Create a LightGBM dataset from the training data\n",
    "train_data = lgb.Dataset(train_x, label=train_y, group=[len(elem) for elem in train_queries.values()])\n",
    "\n",
    "# Set the parameters for the LambdaMART model\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [5],  # Evaluation at NDCG@5\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'device': 'cpu' # Change to GPU if available\n",
    "}\n",
    "\n",
    "# Train the LambdaMART model\n",
    "num_rounds = 100\n",
    "model = lgb.train(params, train_data, num_rounds)\n",
    "\n",
    "# Save the trained model\n",
    "model.save_model('lambda_mart_model.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the LambdaMART model and save the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_write(fn, predictions, queries):\n",
    "\n",
    "    srch_to_ranks = {}\n",
    "    \n",
    "    i=0\n",
    "    for query_id, property_ids in queries.items():\n",
    "        \n",
    "        # Retrieve all probabilities for a certain query ID (assuming the data is still ordered)\n",
    "        ranks = predictions[i:i+len(property_ids)]\n",
    "\n",
    "        # Keep track of the probabilities\n",
    "        srch_to_ranks[query_id] = ranks\n",
    "        i+=len(property_ids)\n",
    "    \n",
    "    # Write predictions to file\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(\"srch_id,prop_id\\n\")\n",
    "\n",
    "        # Iterate over all different queries\n",
    "        for srch_id, srch_probs in srch_to_ranks.items():\n",
    "            \n",
    "            # Get the order of the hotels based on the predicted probabilities\n",
    "            sorted_indices = np.array(srch_probs).argsort()[::-1]\n",
    "            property_ids_sorted = np.array(queries[srch_id])[np.array(sorted_indices)]\n",
    "\n",
    "            for s_id, p_id in zip([srch_id for _ in range(len(property_ids_sorted))],property_ids_sorted):\n",
    "                f.write(f\"{s_id},{p_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -9.68711264  -6.2005793   -9.52642316 ... -12.95762334  -9.81876937\n",
      " -10.64441253]\n"
     ]
    }
   ],
   "source": [
    "# Get the test data\n",
    "test_x, test_queries = generate_lgb_data(lgb_test_sequences, kind=\"test\")\n",
    "\n",
    "predictions = model.predict(test_x, raw_score=False)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Write predictions to file\n",
    "test_and_write(\"test_output/LambdaMART_vanilla.csv\", predictions, test_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
