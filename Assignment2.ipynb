{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:24:58.471166200Z",
     "start_time": "2023-05-20T13:24:54.890373800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:03.545740300Z",
     "start_time": "2023-05-20T13:24:58.468610100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into pandas dataframes. The data has to be manually saved to a folder called 'data'.\n",
    "# Note: the data is quite large, so this may take a while (~40 seconds) if the data is read as a csv. To speed up further file reading, it is converted to pickle format.\n",
    "if (os.path.exists(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    & os.path.exists((os.path.join('data', 'test_set_VU_DM.pickle')))):\n",
    "    train_df = pd.read_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df = pd.read_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))\n",
    "else:\n",
    "    train_df = pd.read_csv(os.path.join('data', 'training_set_VU_DM.csv'))\n",
    "    test_df = pd.read_csv(os.path.join('data', 'test_set_VU_DM.csv'))\n",
    "    train_df.to_pickle(os.path.join('data', 'training_set_VU_DM.pickle'))\n",
    "    test_df.to_pickle(os.path.join('data', 'test_set_VU_DM.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.420803200Z",
     "start_time": "2023-05-20T13:25:03.548383400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of data collection: 2012/11/01 - 2013/06/30\n",
      "Train data contains 4,958,347 rows and 54 columns\n",
      "Test data contains 4,959,183 rows and 50 columns\n",
      "\n",
      "Train data:\n",
      "Number of unique search IDs: 199,795\n",
      "Number of unique property IDs: 129,113\n",
      "Number of clicks per search: avg. 1.11, std. 0.21\n",
      "Number of bookings per search: avg. 0.69, std. 0.16\n",
      "\n",
      "Test data:\n",
      "Number of unique search IDs: 199,549\n",
      "Number of unique property IDs: 129,438\n"
     ]
    }
   ],
   "source": [
    "print(f\"Period of data collection: {pd.to_datetime(train_df['date_time']).min().strftime('%Y/%m/%d')} - {pd.to_datetime(train_df['date_time']).max().strftime('%Y/%m/%d')}\")\n",
    "print(f\"Train data contains {train_df.shape[0]:,} rows and {train_df.shape[1]} columns\")\n",
    "print(f\"Test data contains {test_df.shape[0]:,} rows and {test_df.shape[1]} columns\")\n",
    "print()\n",
    "print(f\"Train data:\")\n",
    "print(f\"Number of unique search IDs: {len(train_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(train_df['prop_id'].unique()):,}\")\n",
    "print(f\"Number of clicks per search: avg. {train_df['click_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['click_bool'].std():.2f}\")\n",
    "print(f\"Number of bookings per search: avg. {train_df['booking_bool'].sum() / len(train_df['srch_id'].unique()):.2f}, std. {train_df['booking_bool'].std():.2f}\")\n",
    "print()\n",
    "print(f\"Test data:\")\n",
    "print(f\"Number of unique search IDs: {len(test_df['srch_id'].unique()):,}\")\n",
    "print(f\"Number of unique property IDs: {len(test_df['prop_id'].unique()):,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Column Name                 | Data Type | Description                                                                                                                                                                                                       |\n",
    "|-----------------------------|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| position                    | Integer   | Hotel position on Expedia's search results page. This is only provided for the training data, but not the test data.                                                                                              |\n",
    "| gross_booking_usd           | Float     | Total value of the transaction. This can differ from the price_usd due to taxes, fees, conventions on multiple day bookings and purchase of a room type other than the one shown in the search                    |\n",
    "| click_bool                  | Boolean   | 1 if the user clicked on the property, 0 if not.                                                                                              |\n",
    "| booking_bool                | Boolean   | 1 if the user booked the property, 0 if not.                    |\n",
    "|                             |           ||\n",
    "| srch_id                     | Integer   | The ID of the search                                                                                                                                                                                              |\n",
    "| date_time                   | Date/time | Date and time of the search                                                                                                                                                                                       |\n",
    "| site_id                     | Integer   | ID of the Expedia point of sale (i.e. Expedia.com, Expedia.co.uk, Expedia.co.jp, ..)                                                                                                                              |\n",
    "| visitor_location_country_id | Integer   | The ID of the country the customer is located                                                                                                                                                                     |\n",
    "| visitor_hist_starrating     | Float     | The mean star rating of hotels the customer has previously purchased; null signifies there is no purchase history on the customer                                                                                 |\n",
    "| visitor_hist_adr_usd        | Float     | The mean price per night (in US$) of the hotels the customer has previously purchased; null signifies there is no purchase history on the customer                                                                |\n",
    "| prop_country_id             | Integer   | The ID of the country the hotel is located in                                                                                                                                                                     |\n",
    "| prop_id                     | Integer   | The ID of the hotel                                                                                                                                                                                               |\n",
    "| prop_starrating             | Integer   | The star rating of the hotel, from 1 to 5, in increments of 1. A 0 indicates the property has no stars, the star rating is not known or cannot be publicized.                                                     |\n",
    "| prop_review_score           | Float     | The mean customer review score for the hotel on a scale out of 5, rounded to 0.5 increments. A 0 means there have been no reviews, null that the information is not available.                                    |\n",
    "| prop_brand_bool             | Integer   | +1 if the hotel is part of a major hotel chain; 0 if it is an independent hotel                                                                                                                                   |\n",
    "| prop_location_score1        | Float     | A (first) score outlining the desirability of a hotel’s location                                                                                                                                                  |\n",
    "| prop_location_score2        | Float     | A (second) score outlining the desirability of the hotel’s location                                                                                                                                               |\n",
    "| prop_log_historical_price   | Float     | The logarithm of the mean price of the hotel over the last trading period. A 0 will occur if the hotel was not sold in that period.                                                                               |\n",
    "| price_usd                   | Float     | Displayed price of the hotel for the given search. Note that different countries have different conventions regarding displaying taxes and fees and the value may be per night or for the whole stay              |\n",
    "| promotion_flag              | Integer   | +1 if the hotel had a sale price promotion specifically displayed                                                                                                                                                 |\n",
    "| srch_destination_id         | Integer   | ID of the destination where the hotel search was performed                                                                                                                                                        |\n",
    "| srch_length_of_stay         | Integer   | Number of nights stay that was searched                                                                                                                                                                           |\n",
    "| srch_booking_window         | Integer   | Number of days in the future the hotel stay started from the search date                                                                                                                                          |\n",
    "| srch_adults_count           | Integer   | The number of adults specified in the hotel room                                                                                                                                                                  |\n",
    "| srch_children_count         | Integer   | The number of (extra occupancy) children specified in the hotel room                                                                                                                                              |\n",
    "| srch_room_count             | Integer   | Number of hotel rooms specified in the search                                                                                                                                                                     |\n",
    "| srch_saturday_night_bool    | Boolean   | +1 if the stay includes a Saturday night, starts from Thursday with a length of stay is less than or equal to 4 nights (i.e. weekend); otherwise 0                                                                |\n",
    "| srch_query_affinity_score   | Float     | The log of the probability a hotel will be clicked on in Internet searches (hence the values are negative)  A null signifies there are no data (i.e. hotel did not register in any searches)                      |\n",
    "| orig_destination_distance   | Float     | Physical distance between the hotel and the customer at the time of search. A null means the distance could not be calculated.                                                                                    |\n",
    "| random_bool                 | Boolean   | +1 when the displayed sort was random, 0 when the normal sort order was displayed                                                                                                                                 |\n",
    "| comp*x*_rate                | Integer   | '*x*' denotes the competitor number. +1 if Expedia has a lower price than competitor 1 for the hotel; 0 if the same; -1 if Expedia’s price is higher than competitor 1; null signifies there is no competitive data |\n",
    "| comp*x*_inv                 | Integer   | '*x*' denotes the competitor number. +1 if competitor 1 does not have availability in the hotel; 0 if both Expedia and competitor 1 have availability; null signifies there is no competitive data                  |\n",
    "| comp*x*_rate_percent_diff   | Float     | '*x*' denotes the competitor number. The absolute percentage difference (if one exists) between Expedia and competitor 1’s price (Expedia’s price the denominator); null signifies there is no competitive data      |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.430549500Z",
     "start_time": "2023-05-20T13:25:05.428475700Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create the following new features: has_starrating, has_review_score, traveling_abroad, srch_prop_country_match, month, and day_of_week\n",
    "    \"\"\"\n",
    "    # has_starrating: boolean whether prop_starrating is 0 or null\n",
    "    df[\"has_starrating\"] = df[\"prop_starrating\"].isnull()\n",
    "    df[\"has_starrating\"] = df[\"has_starrating\"].astype(int)\n",
    "    df.loc[df[\"prop_starrating\"] == 0, \"has_starrating\"] = 1\n",
    "\n",
    "    # has_review_score: boolean whether prop_review_score is 0 or null\n",
    "    df[\"has_review_score\"] = df[\"prop_review_score\"].isnull()\n",
    "    df[\"has_review_score\"] = df[\"has_review_score\"].astype(int)\n",
    "    df.loc[df[\"prop_review_score\"] == 0, \"has_review_score\"] = 1\n",
    "\n",
    "    # traveling_abroad: boolean whether visitor_location_country_id != prop_country_id\n",
    "    df[\"traveling_abroad\"] = df[\"visitor_location_country_id\"] != df[\"prop_country_id\"]\n",
    "    df[\"traveling_abroad\"] = df[\"traveling_abroad\"].astype(int)\n",
    "\n",
    "    # srch_prop_country_match: boolean whether srch_destination_id == prop_country_id\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_destination_id\"] == df[\"prop_country_id\"]\n",
    "    df[\"srch_prop_country_match\"] = df[\"srch_prop_country_match\"].astype(int)\n",
    "\n",
    "    # month: month of the search, one-hot encoded\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"month\"] = df[\"date_time\"].dt.month\n",
    "    df[\"month\"] = df[\"month\"].map({1: \"jan\", 2: \"feb\", 3: \"mar\", 4: \"apr\", 5: \"may\", 6: \"jun\", 7: \"jul\", 8: \"aug\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dec\"})\n",
    "    df = pd.get_dummies(df, columns=[\"month\"], dtype=int)\n",
    "    for col in [\"month_jan\", \"month_feb\", \"month_mar\", \"month_apr\", \"month_may\", \"month_jun\", \"month_jul\", \"month_aug\", \"month_sep\", \"month_oct\", \"month_nov\", \"month_dec\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # day_of_week: day of the week of the search\n",
    "    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n",
    "    df[\"day_of_week\"] = df[\"day_of_week\"].map({0: \"mon\", 1: \"tue\", 2: \"wed\", 3: \"thu\", 4: \"fri\", 5: \"sat\", 6: \"sun\"})\n",
    "    df = pd.get_dummies(df, columns=[\"day_of_week\"], dtype=int)\n",
    "    for col in [\"day_of_week_mon\", \"day_of_week_tue\", \"day_of_week_wed\", \"day_of_week_thu\", \"day_of_week_fri\", \"day_of_week_sat\", \"day_of_week_sun\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.436778600Z",
     "start_time": "2023-05-20T13:25:05.434639400Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_nan_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\", and \"compx_rate_percent_diff\" have >90% NaN values. These values cannot be imputed accurately, so we drop these columns.\n",
    "    \"\"\"\n",
    "    cols = [\"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"srch_query_affinity_score\"] + [f\"comp{i}_rate_percent_diff\" for i in range(1, 9)]\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.464216400Z",
     "start_time": "2023-05-20T13:25:05.439826900Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values for the following columns: \"prop_starrating\", \"prop_review_score\", \"compx_rate\", and \"compx_inv\".\n",
    "\n",
    "    For \"prop_starrating\" and \"prop_review_score\", we replace 0 values with NaN and then impute the NaN values with the mean per srch_id. Remaining NaN values are then filled with 0.\n",
    "    For \"compx_rate\" and \"compx_inv\", we assume that missing data means that Expedia has the same price and equal availability as its competitors. We therefore impute the NaN values with 0.\n",
    "    \"\"\"\n",
    "    # Replace 0 values with NaN\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].replace(0, np.nan)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].replace(0, np.nan)\n",
    "    # Impute NaN values with mean per srch_id\n",
    "    df[\"prop_starrating\"] = df.groupby(\"srch_id\")[\"prop_starrating\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    df[\"prop_review_score\"] = df.groupby(\"srch_id\")[\"prop_review_score\"].transform(lambda x: x.fillna(x.mean()))\n",
    "    # Fill remaining NaN values with 0\n",
    "    df[\"prop_starrating\"] = df[\"prop_starrating\"].fillna(0)\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].fillna(0)\n",
    "\n",
    "    # Impute NaN values with 0\n",
    "    for i in range(1, 9):\n",
    "        df[f\"comp{i}_rate\"] = df[f\"comp{i}_rate\"].fillna(0)\n",
    "        df[f\"comp{i}_inv\"] = df[f\"comp{i}_inv\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.464216400Z",
     "start_time": "2023-05-20T13:25:05.448844300Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_aggregated_values(df):\n",
    "    \"\"\"\n",
    "    Compute the mean, median and standard deviation for the following columns:\n",
    "    \"visitor_hist_starrating\", \"visitor_hist_adr_usd\", \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "    \"\"\"\n",
    "    numerical_cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    # srch_length_of_stay, srch_booking_window, srch_adults_count, srch_children_count, and srch_room_count are also numerical variables, but it has no use aggregating these values over prop_id.\n",
    "\n",
    "    agg_df = df.groupby(\"prop_id\").agg({col: [\"mean\", \"std\", \"median\"] for col in numerical_cols})\n",
    "    agg_df.columns = [\"_\".join(col) for col in agg_df.columns]\n",
    "    agg_df.fillna(0, inplace=True)  # Fill standard deviation NaN values with 0\n",
    "    for col in agg_df.columns:\n",
    "        df[col] = df[\"prop_id\"].map(agg_df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.465229400Z",
     "start_time": "2023-05-20T13:25:05.453456900Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_relative_values(df):\n",
    "    \"\"\"\n",
    "    Subtract the mean per srch_id from the following columns:\n",
    "    \"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"\n",
    "\n",
    "    This is done so that the model can learn the relative values of these columns per srch_id.\n",
    "    \"\"\"\n",
    "    cols = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_log_historical_price\", \"price_usd\"]\n",
    "    grouper = df.groupby('srch_id')\n",
    "    for col in cols:\n",
    "        df[col] = df[col] - grouper[col].transform('mean')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.465229400Z",
     "start_time": "2023-05-20T13:25:05.460127500Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    Train data shows that for \"orig_destination_distance\" over 75% of the data with a calculated value lower than 0.95 of the largest distance was lower than 130, meaning that the distance per srch_id is roughly the same. We assume therefore that this is not a deciding factor for a customer in their booking process and drop this column.\n",
    "\n",
    "    Features were created from \"date_time\" and the column will not be used anymore, so we drop this column as well.\n",
    "\n",
    "    Columns containing IDs (\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"prop_id\", and \"srch_destination_id\") are not used in the model, so we drop these columns as well. \"srch_id\" and \"prop_id\" will remain in the columns for now for later use.\n",
    "\n",
    "    If the supplied dataframe is the training dataframe, drop the unused target columns as well.\n",
    "\n",
    "    # TODO:\n",
    "    I don't really know what to do with \"prop_location_score2\" yet, so I'll drop it for now.\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"orig_destination_distance\"], inplace=True)\n",
    "    df.drop(columns=[\"date_time\"], inplace=True)\n",
    "    df.drop(columns=[\"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"srch_destination_id\"], inplace=True)\n",
    "    df.drop(columns=[\"prop_location_score2\"], inplace=True)\n",
    "    for col in [\"position\", \"gross_bookings_usd\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.487333Z",
     "start_time": "2023-05-20T13:25:05.466250500Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(train_df: pd.DataFrame, test_df: pd.DataFrame, target_value: str = \"booking_bool\"):\n",
    "    \"\"\"\n",
    "    Process the dataframes for training and testing the model.\n",
    "    :param train_df: The training dataframe\n",
    "    :param test_df: The test dataframe\n",
    "    :param target_value: The target value to use for training the model. Either \"booking_bool\" or \"both\".\n",
    "    :return: The processed dataframes\n",
    "    \"\"\"\n",
    "    # Create features\n",
    "    train_df = create_features(train_df)\n",
    "    test_df = create_features(test_df)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    train_df = drop_nan_columns(train_df)\n",
    "    test_df = drop_nan_columns(test_df)\n",
    "\n",
    "    # Impute missing values\n",
    "    train_df = impute_missing_values(train_df)\n",
    "    test_df = impute_missing_values(test_df)\n",
    "\n",
    "    # Compute aggregated values\n",
    "    train_df = compute_aggregated_values(train_df)\n",
    "    test_df = compute_aggregated_values(test_df)\n",
    "\n",
    "    # Compute relative values\n",
    "    train_df = compute_relative_values(train_df)\n",
    "    test_df = compute_relative_values(test_df)\n",
    "\n",
    "    # Drop columns\n",
    "    train_df = drop_columns(train_df)\n",
    "    test_df = drop_columns(test_df)\n",
    "\n",
    "    # Split train data into features and target\n",
    "    if target_value == \"booking_bool\":\n",
    "        train_df[\"target\"] = train_df[\"booking_bool\"]\n",
    "    elif target_value == \"both\":\n",
    "        train_df[\"target\"] = 5*train_df[\"booking_bool\"] + train_df[\"click_bool\"]\n",
    "    else:\n",
    "        raise ValueError(\"target_value must be either 'booking_bool' or 'both'\")\n",
    "\n",
    "    train_df.drop(columns=[\"booking_bool\", \"click_bool\"], inplace=True)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.487333Z",
     "start_time": "2023-05-20T13:25:05.475445200Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        return row[\"srch_id\"], row[\"prop_id\"], torch.tensor(row[\"sequence\"].astype(np.float64)), torch.tensor(row[\"target\"].astype(np.int32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.488354Z",
     "start_time": "2023-05-20T13:25:05.481119800Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        return row[\"srch_id\"], row[\"prop_id\"], torch.tensor(row[\"sequence\"].astype(np.float64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.541334500Z",
     "start_time": "2023-05-20T13:25:05.487333Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences(df: pd.DataFrame, kind: str = \"train\") -> dict:\n",
    "    \"\"\"\n",
    "    Generate sequences from the dataframe.\n",
    "    :param df: The dataframe to take the sequences from.\n",
    "    :param kind: The kind of dataframe. Either \"train\" or \"test\".\n",
    "    :return: Dictionary containing the sequences and targets for the training data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    # Very slow. Feel free to improve if you know how, I'm at a blank.\n",
    "    if kind == \"train\":\n",
    "        idx_dict = df.to_dict(\"index\")\n",
    "        df_vals = df.drop(columns=[\"srch_id\", \"target\"]).values\n",
    "        for i, idx in enumerate(idx_dict):\n",
    "            sequences.append({\"srch_id\": idx_dict[idx][\"srch_id\"], \"prop_id\": idx_dict[idx][\"prop_id\"], \"sequence\": df_vals[i], \"target\": idx_dict[idx][\"target\"]})\n",
    "    elif kind == \"test\":\n",
    "        for srch_id, group in df.groupby(\"srch_id\"):\n",
    "            prop_ids = group[\"prop_id\"].values\n",
    "            sequence = group.drop(columns=[\"srch_id\"]).values\n",
    "            sequences.append({\"srch_id\": srch_id, \"prop_id\": prop_ids, \"sequence\": sequence})\n",
    "    else:\n",
    "        raise ValueError(\"kind must be either 'train' or 'test'\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.541857600Z",
     "start_time": "2023-05-20T13:25:05.497616400Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df: pd.DataFrame, test_df: pd.DataFrame, target_value: str = \"booking_bool\", batch_size: int=1, train_validation_split: float=0.8, shuffle: bool=True, force_data_processing: bool = False) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create dataloaders for the training, validation and test set.\n",
    "    :param train_df: The train dataframe.\n",
    "    :param test_df: The test dataframe.\n",
    "    :param target_value: The target value to use for training the model. Either \"booking_bool\" or \"both\".\n",
    "    :param batch_size: How many samples per batch to load.\n",
    "    :param train_validation_split: The percentage of the dataset to use for training.\n",
    "    :param shuffle: Set to True to have the data reshuffled at every epoch.\n",
    "    :param force_data_processing: Set to True to force the data to be processed again.\n",
    "    :return: Tuple containing the training, validation and test dataloaders.\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join('data', 'final_train_set.pickle')) \\\n",
    "            & os.path.exists(os.path.join('data', 'final_test_set.pickle')) \\\n",
    "            & (not force_data_processing):\n",
    "        train_df = pd.read_pickle(os.path.join('data', f'final_train_set_{target_value}.pickle'))\n",
    "        test_df = pd.read_pickle(os.path.join('data', f'final_test_set_{target_value}.pickle'))\n",
    "    else:\n",
    "        train_df, test_df = process_data(train_df, test_df, target_value=target_value)\n",
    "        train_df.to_pickle(os.path.join('data', f'final_train_set_{target_value}.pickle'))\n",
    "        test_df.to_pickle(os.path.join('data', f'final_test_set_{target_value}.pickle'))\n",
    "\n",
    "    train_sequences = generate_sequences(train_df, kind=\"train\")\n",
    "    train_dataset = TrainDataset(train_sequences)\n",
    "    train_size = int(train_validation_split * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    test_sequences = generate_sequences(test_df, kind=\"test\")\n",
    "    test_dataset = TestDataset(test_sequences)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:25:05.542378700Z",
     "start_time": "2023-05-20T13:25:05.503287400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:37:11.206867400Z",
     "start_time": "2023-05-20T13:25:05.509485600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all data\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(train_df, \n",
    "                                                           test_df,\n",
    "                                                           target_value=\"booking_bool\",\n",
    "                                                           batch_size=1,\n",
    "                                                           train_validation_split=0.8,\n",
    "                                                           shuffle=True,\n",
    "                                                           force_data_processing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:37:11.227874400Z",
     "start_time": "2023-05-20T13:37:11.226855800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate NDCG@k score from file\n",
    "\n",
    "def NDCG_from_file(fn, data, k):\n",
    "    \"\"\"\n",
    "    Function to calculate the NDCG@k score from a txt file in the format specified in the assignment (two \n",
    "    columns, the left one containing the search IDs and the right one containing the PropertyIDs, sorted by\n",
    "    relevance)\n",
    "    \"\"\"\n",
    "    query_to_properties = {}\n",
    "    \n",
    "    # Fill the query-to-properties dictionary\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()[1:] # Remove the header\n",
    "        for line in lines:\n",
    "            query_id, prop_id = line.split(\",\")\n",
    "            query_id, prop_id = int(query_id), int(prop_id)\n",
    "            if query_id not in query_to_properties.keys():\n",
    "                query_to_properties[query_id] = [prop_id]\n",
    "            else:\n",
    "                query_to_properties[query_id].append(prop_id)\n",
    "\n",
    "    NDCG_score = 0\n",
    "    for query_id, prop_ids in query_to_properties.items():\n",
    "        \n",
    "        # TODO: change this to match the correct query\n",
    "        data_query = data[query_id]\n",
    "\n",
    "        # Only select the first k properties\n",
    "        prop_selection = prop_ids[:k]\n",
    "\n",
    "        # TODO: Change \"scores\" to match the name of the column which keeps the scores (5 for booking, 1 for clicking)\n",
    "        true_scores = data.loc(data_query[\"prop_id\"] == prop_selection, \"scores\")\n",
    "    \n",
    "        # Calculate DCG@k (Discounted Cumulative Gain at k)\n",
    "        dcg = np.sum(true_scores / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Sort the true scores in descending order\n",
    "        true_sorted_indices = np.argsort(data_query[\"scores\"])[::-1]\n",
    "        true_sorted_scores = np.array(data_query[\"scores\"])[true_sorted_indices]\n",
    "        \n",
    "        # Calculate ideal DCG@k\n",
    "        ideal_dcg = np.sum(true_sorted_scores[:k] / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Calculate NDCG@k\n",
    "        NDCG_score += dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:37:11.228891200Z",
     "start_time": "2023-05-20T13:37:11.227874400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Neural model\n",
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural prediction model for predicting the probability that a user will buy \n",
    "    a room at a certain hotel.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_size=64, dropout_bool=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(num_features)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dropout_bool = dropout_bool\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        self.fc3 = nn.Linear(hidden_size//4, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.layer_norm(features.float())\n",
    "        x = self.embedding(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        if self.dropout_bool:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        if self.dropout_bool:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        if self.dropout_bool:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Training and testing code_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:37:11.247518700Z",
     "start_time": "2023-05-20T13:37:11.232008200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, criterion, num_epochs, learning_rate, train_loader, val_loader, device):\n",
    "    train_loss = []\n",
    "    val_loss = []    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Now training epoch {epoch}\")\n",
    "\n",
    "        # Training loop\n",
    "        running_train_loss = 0\n",
    "        for _, _, features, targets in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features.squeeze().to(device))\n",
    "            loss = criterion(outputs.to(device).squeeze().float(), targets.to(device).squeeze().float())\n",
    "            loss.backward()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(running_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        running_val_loss = 0\n",
    "        for _, _, features, targets in val_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(features.to(device))\n",
    "                loss = criterion(outputs.to(device).squeeze().float(), targets.to(device).squeeze().float())\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        val_loss.append(running_val_loss)\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def test_and_write(fn, model, loader):\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(\"SearchId, PropertyId\\n\")\n",
    "        for elem in tqdm(loader):\n",
    "\n",
    "            # Check if there are four elements in the dataloader (a feature vector and a label).\n",
    "            # If so, we have the validation set and need to extract the features accordingly\n",
    "            if len(elem) == 4:\n",
    "                search_id, prop_ids, features, _ = elem\n",
    "            else:\n",
    "                search_id, prop_ids, features = elem \n",
    "\n",
    "            # This assumes that features is batched and shaped (num_properties x num_features)\n",
    "            # search_id = features[:, 0] #TODO extract search ID from feature vector correctly\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Don't take into account the search ID (first feature)\n",
    "                outputs = model(features.squeeze())\n",
    "                probabilities = torch.sigmoid(outputs.squeeze())\n",
    "                sorted_indices = probabilities.argsort(descending=True)\n",
    "\n",
    "            property_ids = prop_ids.squeeze()[sorted_indices]\n",
    "\n",
    "            for search, prop in zip([search_id.item() for _ in range(len(property_ids))], list(property_ids)):\n",
    "                f.write(f\"{search}, {prop}\\n\")\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T13:37:13.718480200Z",
     "start_time": "2023-05-20T13:37:11.339782100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Now training simple MLP model (without dropout)\n",
      "Now training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4995 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [25] at entry 0 and [32] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining on device: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNow training simple MLP model (without dropout)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m nn_train_loss, nn_val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43msimple_nn_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msimple_nn_criterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdev_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[19], line 17\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, criterion, num_epochs, learning_rate, train_loader, val_loader, device)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[0;32m     16\u001B[0m running_train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, _, features, targets \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader):\n\u001B[0;32m     18\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     20\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(features\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mto(device))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtransposed\u001B[49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001B[0m, in \u001B[0;36mcollate_numpy_array_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    160\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    161\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [25] at entry 0 and [32] at entry 1"
     ]
    }
   ],
   "source": [
    "# Train simple NN model\n",
    "num_epochs = 50\n",
    "hidden_dim = 128\n",
    "num_features = 69\n",
    "learning_rate =  3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "simple_nn_model = RecommenderNet(num_features, hidden_size=hidden_dim, dropout_bool=False)\n",
    "simple_nn_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Now training simple MLP model (without dropout)\")\n",
    "nn_train_loss, nn_val_loss = train(simple_nn_model, simple_nn_criterion, num_epochs, learning_rate, train_loader, dev_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(num_epochs), nn_train_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.savefig(\"simple_nn_training_loss.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(range(num_epochs), nn_val_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.savefig(\"simple_nn_validation_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and save results of simple NN model\n",
    "if not os.path.isdir(\"test_output\"):\n",
    "    os.makedirs(\"test_output\")\n",
    "\n",
    "# For validation set\n",
    "test_and_write(\"test_output/simple_nn_val.txt\", simple_nn_model, dev_loader)\n",
    "# And for testing set\n",
    "test_and_write(\"test_output/simple_nn_test.txt\", simple_nn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_model_dropout = RecommenderNet(num_features, hidden_size=hidden_dim, dropout_bool=True)\n",
    "simple_nn_criterion_dropout = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Now training simple MLP model (with dropout)\")\n",
    "nn_train_loss_dropout, nn_val_loss_dropout = train(simple_nn_model_dropout, simple_nn_criterion_dropout, num_epochs, learning_rate, train_loader, dev_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(num_epochs), nn_train_loss_dropout)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.savefig(\"simple_nn_training_loss_dropout.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(range(num_epochs), nn_val_loss_dropout)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.savefig(\"simple_nn_validation_loss_dropout.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save results for the dropout model\n",
    "# For validation set\n",
    "test_and_write(\"test_output/simple_nn_dropout_val.txt\", simple_nn_model_dropout, dev_loader)\n",
    "# And for testing set\n",
    "test_and_write(\"test_output/simple_nn_dropout_test.txt\", simple_nn_model_dropout, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDCG score for validation set\n",
    "# nn_score = NDCG_from_file(\"simple_nn_val.txt\", train_df, 5)\n",
    "# print(f\"NDGC score for the simple MLP (on validation set): {nn_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
